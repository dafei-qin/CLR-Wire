# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# Copyright Lightning AI. Licensed under the Apache License 2.0,
# see LICENSE file at https://github.com/Lightning-AI/litgpt/blob/main/LICENSE

import glob
import math
import sys
import time
from pathlib import Path
from typing import List, Optional, Tuple, Union
import math
import lightning as L
import torch
from lightning.fabric.strategies import FSDPStrategy
from torch.utils.data import DataLoader
from functools import partial
# support running without installing as a package
wd = Path(__file__).parent.parent.resolve()
sys.path.append(str(wd))
# from apex.optimizers import FusedAdam #torch optimizer has a cuda backend, which is faster actually
from lit_gpt.model import GPT, Block, Config
from lit_gpt.packed_dataset import CombinedDataset, PackedDataset
from sft.datasets.DatasetDEEMOS import Sample_Dataset
from lit_gpt.speed_monitor import SpeedMonitorFabric as Monitor
from lit_gpt.speed_monitor import estimate_flops
from lit_gpt.utils import chunked_cross_entropy, num_parameters
from pytorch_lightning.loggers import WandbLogger
from lit_gpt import FusedCrossEntropyLoss
from sft.datasets.serializaitonDEEMOS import deserialize
import random
import os
from datetime import datetime
import numpy as np
import trimesh

import warnings
warnings.filterwarnings("ignore", message="When using.*NO_SHARD.*")
# ---------------------------
# EMA ç»Ÿè®¡ç±»ï¼šç”¨äº CD loss çš„ç¨³å¥å½’ä¸€åŒ–
# ---------------------------
class EMAStats:
    """
    ç”¨äºè·Ÿè¸ª CD loss çš„ EMA å‡å€¼å’Œæ ‡å‡†å·®ï¼Œåšç¨³å¥å½’ä¸€åŒ–ã€‚
    
    å½’ä¸€åŒ–æ–¹å¼ï¼šz-score (z = (x - Î¼) / Ïƒ)
    - z â‰ˆ 0ï¼šCD æ¥è¿‘å¹³å‡æ°´å¹³
    - z > 0ï¼šCD é«˜äºå¹³å‡ï¼ˆmesh è´¨é‡è¾ƒå·®ï¼‰â†’ éœ€è¦æ›´é«˜çš„ CE loss æƒé‡
    - z < 0ï¼šCD ä½äºå¹³å‡ï¼ˆmesh è´¨é‡è¾ƒå¥½ï¼‰â†’ å¯ä»¥é™ä½ CE loss æƒé‡
    """
    def __init__(self, momentum=0.99, eps=1e-8):
        self.momentum = momentum
        self.eps = eps
        self.mean = None
        self.var = None
        self.count = 0
    
    def update(self, values):
        """æ›´æ–° EMA ç»Ÿè®¡é‡ï¼Œvalues æ˜¯ä¸€ä¸ª list æˆ– numpy array"""
        if len(values) == 0:
            return
        
        batch_mean = np.mean(values)
        batch_var = np.var(values) if len(values) > 1 else 0.0
        
        if self.mean is None:
            # é¦–æ¬¡æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨ batch ç»Ÿè®¡é‡
            self.mean = batch_mean
            self.var = batch_var
        else:
            # EMA æ›´æ–°
            self.mean = self.momentum * self.mean + (1 - self.momentum) * batch_mean
            self.var = self.momentum * self.var + (1 - self.momentum) * batch_var
        
        self.count += len(values)
    
    def normalize(self, values):
        """
        å¯¹ values åš z-score å½’ä¸€åŒ–
        
        Args:
            values: list æˆ– numpy arrayï¼ŒåŸå§‹ CD å€¼
        
        Returns:
            numpy arrayï¼Œå½’ä¸€åŒ–åçš„ z å€¼
        """
        if self.mean is None:
            # ç»Ÿè®¡é‡è¿˜æœªåˆå§‹åŒ–ï¼Œè¿”å› 0ï¼ˆä¸­æ€§æƒé‡ï¼‰
            return np.zeros_like(np.array(values), dtype=np.float32)
        
        std = np.sqrt(self.var + self.eps)
        return (np.array(values) - self.mean) / std
    
    def get_stats(self):
        """è¿”å›å½“å‰ EMA ç»Ÿè®¡é‡ (mean, std)"""
        if self.mean is None:
            return 0.0, 1.0
        return self.mean, np.sqrt(self.var + self.eps)
    
    def sync_across_ranks(self, fabric):
        """
        è·¨ GPU åŒæ­¥ EMA ç»Ÿè®¡é‡ï¼ˆåªåœ¨ rank 0 ç»´æŠ¤ï¼Œå¹¿æ’­ç»™å…¶ä»– rankï¼‰
        
        Args:
            fabric: Lightning Fabric å¯¹è±¡
        
        æ³¨æ„ï¼šæ‰€æœ‰ rank å¿…é¡»éƒ½å‚ä¸ broadcastï¼Œå¦åˆ™ä¼š deadlock
        """
        if fabric.world_size <= 1:
            return  # å•å¡ä¸éœ€è¦åŒæ­¥
        
        # å…ˆå¹¿æ’­ä¸€ä¸ª flag è¡¨ç¤º rank0 æ˜¯å¦æœ‰æœ‰æ•ˆç»Ÿè®¡é‡
        has_stats = (self.mean is not None) if fabric.global_rank == 0 else False
        flag = torch.tensor([1 if has_stats else 0], device=fabric.device, dtype=torch.int32)
        torch.distributed.broadcast(flag, src=0)
        
        if flag.item() == 0:
            # rank0 ä¹Ÿæ²¡æœ‰ statsï¼šæ‰€æœ‰äººéƒ½æ¸…ç©º
            self.mean, self.var = None, None
            return
        
        # rank0 æä¾›çœŸå®å€¼ï¼Œå…¶å®ƒ rank ç”¨å ä½ï¼ˆä½†ä¹Ÿè¦å‚ä¸ broadcastï¼‰
        if fabric.global_rank == 0:
            stats_tensor = torch.tensor([self.mean, self.var], device=fabric.device, dtype=torch.float32)
        else:
            stats_tensor = torch.zeros(2, device=fabric.device, dtype=torch.float32)
        
        torch.distributed.broadcast(stats_tensor, src=0)
        
        # æ›´æ–°æœ¬åœ°ç»Ÿè®¡é‡
        self.mean = stats_tensor[0].item()
        self.var = stats_tensor[1].item()


def compute_cd_weights(z_scores, w_min=0.5, w_max=2.0, k=1.0, normalize_mean=True):
    """
    å°† z-score æ˜ å°„åˆ°æœ‰ç•Œçš„æƒé‡åŒºé—´ [w_min, w_max]
    
    ä½¿ç”¨ sigmoid å‡½æ•°å®ç°å¹³æ»‘çš„æƒé‡æ˜ å°„ï¼š
    - z > 0ï¼ˆCD é«˜äºå¹³å‡ï¼Œè´¨é‡å·®ï¼‰â†’ æƒé‡æ›´å¤§ â†’ CE loss æ›´é‡è¦
    - z < 0ï¼ˆCD ä½äºå¹³å‡ï¼Œè´¨é‡å¥½ï¼‰â†’ æƒé‡æ›´å° â†’ CE loss å¯ä»¥æ”¾æ¾
    - z = 0ï¼ˆCD ç­‰äºå¹³å‡ï¼‰â†’ æƒé‡ = (w_min + w_max) / 2
    
    Args:
        z_scores: numpy arrayï¼Œå½’ä¸€åŒ–åçš„ CD z-score
        w_min: æœ€å°æƒé‡ï¼ˆCD å¥½æ—¶ï¼‰ï¼Œé»˜è®¤ 0.5
        w_max: æœ€å¤§æƒé‡ï¼ˆCD å·®æ—¶ï¼‰ï¼Œé»˜è®¤ 2.0
        k: sigmoid çš„é™¡å³­ç¨‹åº¦ï¼Œè¶Šå¤§è¶Šé™¡ï¼Œé»˜è®¤ 1.0
        normalize_mean: æ˜¯å¦å°†æƒé‡å‡å€¼å½’ä¸€åŒ–åˆ° 1ï¼ˆä¿æŒæ•´ä½“ loss scale ç¨³å®šï¼‰
    
    Returns:
        numpy arrayï¼Œæƒé‡å€¼
        - å¦‚æœ normalize_mean=Trueï¼Œæƒé‡å‡å€¼ä¸º 1.0
        - å¦‚æœ normalize_mean=Falseï¼Œæƒé‡èŒƒå›´ä¸º [w_min, w_max]
    """
    z_scores = np.array(z_scores, dtype=np.float32)
    # sigmoid æ˜ å°„ï¼šz=0 â†’ 0.5, z>0 â†’ æ¥è¿‘1, z<0 â†’ æ¥è¿‘0
    sigmoid_z = 1.0 / (1.0 + np.exp(-k * np.clip(z_scores, -10, 10)))  # clip é˜²æ­¢ overflow
    # æ˜ å°„åˆ° [w_min, w_max] åŒºé—´
    weights = w_min + (w_max - w_min) * sigmoid_z
    
    # å½’ä¸€åŒ–æƒé‡å‡å€¼åˆ° 1ï¼Œä¿æŒæ•´ä½“ loss scale ç¨³å®š
    # è¿™æ ·ä¸ä¼šå½±å“ grad_clip å’Œå­¦ä¹ ç‡çš„æœ‰æ•ˆæ€§
    if normalize_mean and len(weights) > 0:
        weight_mean = weights.mean()
        if weight_mean > 0:
            weights = weights / weight_mean
    
    return weights


model_name = "Diff_LLaMA_551M" # change to "Samba_1.3B" for 1.3B model
train_config = "HY1024_tsz128x16k_100B_ScaleUp20k_unlockCondition" # chanage to "tsz512x4k_100B" for 1.3B model
name = train_config +"_" + model_name

out_dir = Path(os.getenv("LIGHTNING_ARTIFACTS_DIR", "out")) / name / f"Samba-DEEMOS-{datetime.now().strftime('%m-%d-%H')}"
# Token ID å®šä¹‰ï¼ˆä¸å¼•å…¥æ–° tokenï¼Œä¿æŒä¸æ—§ ckpt å…¼å®¹ï¼‰
BOS_TOKEN_ID = 4736  # åºåˆ—å¼€å§‹ token
EOS_TOKEN_ID = 4737  # åºåˆ—ç»“æŸ tokenï¼ˆä¹Ÿç”¨äº padding å¡«å……ï¼‰
# æ³¨æ„ï¼šä¸ä½¿ç”¨ä¸“ç”¨ PAD tokenï¼Œè€Œæ˜¯ç”¨ EOS å¡«å…… + ä½ç½® mask æ¥å®ç° padding ä¸å‚ä¸ loss
devices = torch.cuda.device_count() or 1
# æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰çš„ Sample_Datasetï¼ˆä¸åš paddingï¼Œå˜é•¿æ ·æœ¬ä»¥ list ç»„ç»‡ï¼‰
use_sample_dataset = True

# ========== ShapeVAE Conditioner è®­ç»ƒé…ç½® ==========
# æ˜¯å¦å†»ç»“ conditionerï¼ˆFalse è¡¨ç¤ºè§£é”è®­ç»ƒï¼‰
freeze_conditioner = False
# conditioner çš„å­¦ä¹ ç‡å€ç‡ï¼ˆç›¸å¯¹äºä¸»å­¦ä¹ ç‡ï¼‰
# é€šå¸¸ pretrained æ¨¡å‹å¾®è°ƒæ—¶ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
conditioner_lr_scale = 1.0

# ========== Chamfer Distance (CD) Loss ç›‘æ§é…ç½® ==========
# æ˜¯å¦è®¡ç®— CD loss ç”¨äºç›‘æ§ï¼ˆCPU/GPU å¯†é›†ï¼Œä¼šé™ä½è®­ç»ƒé€Ÿåº¦çº¦ 10-20%ï¼‰
compute_cd_loss = False
# CD loss è¯¦ç»†æ‰“å°é—´éš”ï¼ˆæ­¥æ•°ï¼‰ï¼Œå»ºè®® 100 æˆ–æ›´å¤§
cd_loss_log_interval = 100

# ========== Checkpoint é…ç½® ==========
# FSDP state_dict ç±»å‹ï¼š
#   - "full": å®Œæ•´ state dictï¼ˆå…¼å®¹æ€§å¥½ï¼Œä½†ä¿å­˜/åŠ è½½æ…¢ï¼Œ20G ckpt å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
#   - "sharded": åˆ†ç‰‡ state dictï¼ˆå¿«ï¼Œä½† checkpoint åˆ†æ•£åœ¨å¤šä¸ªæ–‡ä»¶ï¼‰
# æ³¨æ„ï¼šåˆ‡æ¢ç±»å‹åï¼Œæ—§çš„ checkpoint å¯èƒ½æ— æ³•åŠ è½½
fsdp_state_dict_type = "full"  # é»˜è®¤ä½¿ç”¨ full ä¿æŒå…¼å®¹æ€§

# Hyperparameters
if "20B" in name:
    # single node
    nodes = 1 # max 8
    max_tokens = int(1e11) // 5 # 20 billion
elif "100B" in name:
    # multi-node
    nodes = 8 # max 8
    max_tokens = int(1e11) # 100 billion

if "512x4k" in name:
    #4k
    global_batch_size = 512 // nodes
    micro_batch_size = 6
elif "256x8k" in name:
    #8k
    global_batch_size = 256 // nodes
    micro_batch_size = 4
elif "128x16k" in name:
    #16k
    global_batch_size = 320 // nodes
    micro_batch_size = 5
elif "64x32k" in name:
    #32k
    global_batch_size = 64 // nodes
    micro_batch_size = 1
elif "1024x2k" in name:
    #2k
    global_batch_size = 1024 // nodes
    micro_batch_size = 16

# overfit
# global_batch_size = 32

learning_rate = 1e-4

total_evals = 400
warmup_tokens = int(max_tokens * 0.05)
log_step_interval = 10
eval_iters = total_evals // micro_batch_size # 50 # 25
save_step_interval = 2500  # 500
eval_step_interval = 100000000000000

num_extrapol = 4

weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
decay_lr = True
min_lr = 1e-5

# è®­ç»ƒæ€»è½®æ•°ï¼ˆæŒ‰ epoch è®¡æ•°ï¼‰
num_epochs = 20
# num_epochs = 3000

batch_size = global_batch_size // devices
gradient_accumulation_steps = batch_size // micro_batch_size
assert gradient_accumulation_steps > 0

# log_iter_interval = log_step_interval * gradient_accumulation_steps
log_iter_interval = log_step_interval

# Treat all dataset equally by their size. If you want to use a different weight for a dataset, add it to the list with the weight.
train_data_config = [
    ("train_slim", 1.0),
]

val_data_config = [
    ("validation", 1.0),
]

hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str, bool)) and not k.startswith("_")}

wandb_logger = WandbLogger(project="Pretrain-LLM-Hourglass-551M-DEEMOS", entity="ruixu-hku")

# ---------------------------
# Chamfer Distance è®¡ç®—å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
# ---------------------------
def sample_points_from_mesh(vertices, faces, num_samples=1024):
    """
    ä» mesh è¡¨é¢éšæœºé‡‡æ ·ç‚¹
    
    Args:
        vertices: (N, 3) numpy array
        faces: (M, 3) numpy array
        num_samples: é‡‡æ ·ç‚¹æ•°
    
    Returns:
        sampled_points: (num_samples, 3) numpy array
    """
    try:
        # åˆ›å»º trimesh å¯¹è±¡
        mesh = trimesh.Trimesh(vertices=vertices, faces=faces, process=False)
        
        # ä»è¡¨é¢é‡‡æ ·ç‚¹
        sampled_points, _ = trimesh.sample.sample_surface(mesh, num_samples)
        
        return sampled_points
    except Exception as e:
        # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨é¡¶ç‚¹å¹¶éšæœºé‡‡æ ·
        if len(vertices) >= num_samples:
            indices = np.random.choice(len(vertices), num_samples, replace=False)
            return vertices[indices]
        else:
            # é¡¶ç‚¹ä¸å¤Ÿï¼Œé‡å¤é‡‡æ ·
            indices = np.random.choice(len(vertices), num_samples, replace=True)
            return vertices[indices]


def compute_chamfer_distance_fast(pred_points, gt_points, device=None):
    """
    å¿«é€Ÿè®¡ç®—ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„ Chamfer Distance
    
    Args:
        pred_points: (N, 3) numpy array æˆ– torch tensor
        gt_points: (M, 3) numpy array æˆ– torch tensor
        device: ç›®æ ‡è®¾å¤‡ï¼ˆå¦‚ fabric.deviceï¼‰ï¼Œé¿å…å¤šå¡ç¯å¢ƒä¸‹è®¾å¤‡ä¸åŒ¹é…
    
    Returns:
        chamfer_dist: åŒå‘ Chamfer Distance
    """
    try:
        # è½¬æ¢ä¸º torch tensor
        if not torch.is_tensor(pred_points):
            pred_points = torch.from_numpy(pred_points).float()
        if not torch.is_tensor(gt_points):
            gt_points = torch.from_numpy(gt_points).float()
        
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆé¿å…å¤šå¡ç¯å¢ƒä¸‹è®¾å¤‡ä¸åŒ¹é…ï¼‰
        if device is not None:
            pred_points = pred_points.to(device)
            gt_points = gt_points.to(device)
        elif not pred_points.is_cuda:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè®¾å¤‡ä¸”ä¸åœ¨ GPU ä¸Šï¼Œä½¿ç”¨ CPU è®¡ç®—
            pass  # ä¿æŒåœ¨ CPU ä¸Š
        
        # è®¡ç®—åŒå‘ Chamfer Distance
        # pred -> gt
        dist_matrix = torch.cdist(pred_points.unsqueeze(0), gt_points.unsqueeze(0), p=2).squeeze(0)  # (N, M)
        min_dist_pred_to_gt = dist_matrix.min(dim=1)[0]  # (N,)
        
        # gt -> pred
        min_dist_gt_to_pred = dist_matrix.min(dim=0)[0]  # (M,)
        
        # Chamfer Distance (åŒå‘å¹³å‡)
        chamfer_dist = (min_dist_pred_to_gt.mean() + min_dist_gt_to_pred.mean()).item()
        
        return chamfer_dist
    
    except Exception as e:
        return float('inf')


def validate_and_filter_faces(vertices, faces):
    """éªŒè¯å¹¶è¿‡æ»¤ facesï¼Œç§»é™¤åŒ…å«è¶…å‡º vertices èŒƒå›´ç´¢å¼•çš„ face"""
    if len(vertices) == 0 or len(faces) == 0:
        return np.array([])
    
    num_vertices = len(vertices)
    max_valid_idx = num_vertices - 1
    valid_mask = np.all((faces >= 0) & (faces <= max_valid_idx), axis=1)
    
    filtered_faces = faces[valid_mask]
    return filtered_faces


def tokens_to_mesh_with_sampling(tokens, num_samples=1024, debug=False):
    """
    å°† token åºåˆ—è§£ç ä¸º meshï¼Œå¹¶ä»è¡¨é¢é‡‡æ ·ç‚¹
    
    Args:
        tokens: token åºåˆ— (numpy array æˆ– torch tensor)ï¼Œåº”è¯¥å·²ç»æŒ‰é•¿åº¦æˆªæ–­å¥½
        num_samples: é‡‡æ ·ç‚¹æ•°
        debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
    
    Returns:
        sampled_points: (num_samples, 3) numpy arrayï¼Œå¦‚æœè§£ç å¤±è´¥è¿”å› None
    
    Note:
        - 4736 (BOS_TOKEN_ID): åºåˆ—å¼€å§‹ token
        - 4737 (EOS_TOKEN_ID): åºåˆ—ç»“æŸ token
        - è°ƒç”¨å‰åº”è¯¥å·²ç»æŒ‰çœŸå®é•¿åº¦æˆªæ–­ï¼Œä¸éœ€è¦å†è¿‡æ»¤ padding
    """
    try:
        # è½¬ä¸º numpy
        if torch.is_tensor(tokens):
            tokens = tokens.detach().cpu().numpy()
        
        if len(tokens) == 0:
            if debug:
                print(f"  âŒ Empty token sequence")
            return None
        
        # ç¡®ä¿åºåˆ—æ ¼å¼æ­£ç¡®ï¼šå‰é¢æœ‰ BOS (4736)ï¼Œæœ«å°¾æœ‰ EOS (4737)
        # å¦‚æœå¼€å¤´ä¸æ˜¯ BOSï¼Œæ·»åŠ å®ƒ
        if tokens[0] != BOS_TOKEN_ID:
            tokens = np.concatenate([[BOS_TOKEN_ID], tokens])
        
        # å¦‚æœæœ«å°¾ä¸æ˜¯ EOSï¼Œæ·»åŠ å®ƒ
        if tokens[-1] != EOS_TOKEN_ID:
            tokens = np.concatenate([tokens, [EOS_TOKEN_ID]])
        
        if debug:
            print(f"  ğŸ“Š Token sequence: len={len(tokens)}, starts with {tokens[0]}, ends with {tokens[-1]}")
        
        # è§£ç ä¸º mesh
        vertices, faces = deserialize(tokens)
        
        if debug:
            print(f"  ğŸ“Š Decoded: {len(vertices)} vertices, {len(faces)} faces (raw)")
        
        if len(vertices) == 0:
            if debug:
                print(f"  âŒ No vertices after deserialize")
            return None
        
        # è¿‡æ»¤æ— æ•ˆçš„ faces
        faces = faces.reshape(-1, 3)
        faces = validate_and_filter_faces(vertices, faces)
        
        if debug:
            print(f"  ğŸ“Š After filtering: {len(faces)} valid faces")
        
        if len(faces) == 0:
            # æ²¡æœ‰æœ‰æ•ˆçš„ faceï¼Œç›´æ¥ä½¿ç”¨é¡¶ç‚¹
            if len(vertices) >= num_samples:
                indices = np.random.choice(len(vertices), num_samples, replace=False)
                return vertices[indices]
            else:
                indices = np.random.choice(len(vertices), num_samples, replace=True)
                return vertices[indices]
        
        # ä»è¡¨é¢é‡‡æ ·ç‚¹
        sampled_points = sample_points_from_mesh(vertices, faces, num_samples)
        
        if debug:
            print(f"  âœ… Successfully sampled {len(sampled_points)} points")
        
        return sampled_points
    
    except Exception as e:
        if debug:
            print(f"  âŒ Exception: {e}")
            import traceback
            traceback.print_exc()
        return None

# ---------------------------
# æ–°å¢ï¼šä¸¥æ ¼æ¬è¿æ¨¡å—ï¼ˆå«æœªæ³¨å†Œ tensorï¼‰
# ---------------------------
def move_module_strict(module: torch.nn.Module, device: torch.device, dtype: torch.dtype | None = None):
    """æŠŠ module çš„å‚æ•°ã€buffers ä»¥åŠæœªæ³¨å†Œçš„è£¸ tensor å±æ€§éƒ½æ¬åˆ° device/dtypeã€‚"""
    module.to(device=device, dtype=dtype)
    for sub in module.modules():
        param_names = set(n for n, _ in sub.named_parameters(recurse=False))
        buffer_names = set(n for n, _ in sub.named_buffers(recurse=False))
        for name, value in vars(sub).items():
            if name in param_names or name in buffer_names:
                continue
            if isinstance(value, torch.Tensor):
                setattr(sub, name, value.to(device=device, dtype=(dtype or value.dtype)))


# ---------------------------
# Conditioner Checkpoint è¾…åŠ©å‡½æ•°ï¼ˆç»Ÿä¸€ä¿å­˜/åŠ è½½ï¼‰
# ---------------------------
def save_checkpoint_with_conditioner(fabric, checkpoint_path: Path, state: dict) -> None:
    """
    ä¿å­˜ checkpointï¼ŒåŒ…å«ä¸»æ¨¡å‹å’Œ conditionerã€‚
    ç”±äº conditioner åœ¨ FSDP ignored_modules ä¸­ï¼Œéœ€è¦æ‰‹åŠ¨ä¿å­˜ã€‚
    """
    model = state["model"]
    raw_model = model.module if hasattr(model, 'module') else model
    
    # åœ¨ä¿å­˜å‰ï¼Œæ‰‹åŠ¨å°† conditioner çš„ state_dict åŠ å…¥åˆ° state ä¸­
    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
        # åªåœ¨ rank 0 å‡†å¤‡ conditioner state
        if fabric.global_rank == 0:
            state['conditioner_state_dict'] = raw_model.conditioner.state_dict()
            state['freeze_conditioner'] = freeze_conditioner
    
    # ä¿å­˜å®Œæ•´çš„ stateï¼ˆåŒ…å« conditionerï¼‰
    fabric.save(checkpoint_path, state)
    
    # æ¸…ç†ä¸´æ—¶æ·»åŠ çš„ key
    if 'conditioner_state_dict' in state:
        del state['conditioner_state_dict']
    if 'freeze_conditioner' in state:
        del state['freeze_conditioner']
    
    if fabric.global_rank == 0:
        fabric.print(f"ğŸ’¾ Checkpoint saved to {str(checkpoint_path)!r}")
        if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
            fabric.print(f"   âœ… Conditioner included in checkpoint")


def load_checkpoint_with_conditioner(fabric, checkpoint_path: Path, state: dict, model) -> None:
    """
    åŠ è½½ checkpointï¼ŒåŒ…å«ä¸»æ¨¡å‹å’Œ conditionerã€‚
    """
    # åŠ è½½ checkpoint
    fabric.load(checkpoint_path, state)
    
    raw_model = model.module if hasattr(model, 'module') else model
    
    # å°è¯•åŠ è½½ conditionerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
        # æ£€æŸ¥ checkpoint ä¸­æ˜¯å¦æœ‰ conditioner
        if 'conditioner_state_dict' in state:
            if fabric.global_rank == 0:
                fabric.print(f"ğŸ“‚ Loading conditioner from checkpoint...")
            
            conditioner_state = state['conditioner_state_dict']
            saved_freeze = state.get('freeze_conditioner', True)
            
            # å¹¿æ’­ç»™æ‰€æœ‰ ranksï¼ˆå¦‚æœæ˜¯å¤šGPUï¼‰
            if fabric.world_size > 1:
                object_list = [conditioner_state]
                torch.distributed.broadcast_object_list(object_list, src=0)
                conditioner_state = object_list[0]
            
            # åŠ è½½ state_dict
            raw_model.conditioner.load_state_dict(conditioner_state)
            
            # ç¡®ä¿ç²¾åº¦ä¸€è‡´ï¼ˆè½¬ä¸º fp32ï¼‰
            if next(raw_model.conditioner.parameters()).device != fabric.device:
                move_module_strict(raw_model.conditioner, fabric.device)
            
            for param in raw_model.conditioner.parameters():
                if param.dtype != torch.float32:
                    param.data = param.data.to(torch.float32)
            
            for buffer in raw_model.conditioner.buffers():
                if buffer.dtype not in [torch.long, torch.int, torch.bool]:
                    if buffer.dtype != torch.float32:
                        buffer.data = buffer.data.to(torch.float32)
            
            # æ¸…ç†ä¸´æ—¶ key
            del state['conditioner_state_dict']
            if 'freeze_conditioner' in state:
                del state['freeze_conditioner']
            
            if fabric.global_rank == 0:
                fabric.print(f"âœ… Conditioner loaded successfully!")
                fabric.print(f"   â””â”€â”€ Was saved with freeze_conditioner={saved_freeze}, current={freeze_conditioner}")
        else:
            if fabric.global_rank == 0:
                fabric.print(f"âš ï¸  No conditioner found in checkpoint")
                fabric.print(f"   ShapeVAE will use default initialization (train from scratch or resume)")

def setup(
    train_data_dir: Path = Path("data/redpajama_sample"),
    val_data_dir: Optional[Path] = None,
    resume: Union[bool, Path] = False,
    warm_start_ckpt: Optional[Path] = None, 
) -> None:
    # 1) å…ˆåœ¨ CPU ä¸Šæ„é€ æ¨¡å‹ï¼ˆæ„é€ æœŸä¸è¦æŒ‡å®š deviceï¼‰
    config = Config.from_name(model_name)
    config.padded_vocab_size = (2*4**3) + (8**3) + (16**3) + 1 + 1  # 4736 + 2
    config.block_size = 270000

    # æ ¹æ® freeze_conditioner é…ç½®å†³å®šæ˜¯å¦å†»ç»“ conditioner
    model = GPT(config, freeze_conditioner=freeze_conditioner)
    model.apply(partial(model._init_weights, n_layer=config.n_layer))

    # å¯é€‰ï¼šä»æ—§ckptè¿›è¡Œwarm-startï¼Œä»…åŠ è½½åŒ¹é…æƒé‡ï¼ˆå¿½ç•¥å¤šå‡ºæ¥çš„æ–°å±‚ï¼‰
    if warm_start_ckpt is not None:
        try:
            ckpt = torch.load(warm_start_ckpt, map_location="cpu")
            model_state = ckpt.get("model", ckpt)
            missing, unexpected = model.load_state_dict(model_state, strict=False)
            print(f"Warm-start loaded with strict=False. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
        except Exception as e:
            print(f"Warm-start failed: {e}")

    # 2) æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å†»ç»“ conditioner
    if hasattr(model, "conditioner") and isinstance(model.conditioner, torch.nn.Module):
        if freeze_conditioner:
            # å†»ç»“ conditioner
            for p in model.conditioner.parameters():
                p.requires_grad = False
            model.conditioner.eval()
            print("ğŸ”’ Conditioner is FROZEN (not trainable)")
        else:
            # è§£é” conditionerï¼Œä¿æŒ train æ¨¡å¼
            for p in model.conditioner.parameters():
                p.requires_grad = True
            # é€’å½’è®¾ç½®æ‰€æœ‰å­æ¨¡å—ä¸º train æ¨¡å¼ï¼ˆåŒ…æ‹¬ BatchNorm/LayerNormï¼‰
            def set_train_recursive(module):
                module.train()
                for child in module.children():
                    set_train_recursive(child)
            set_train_recursive(model.conditioner)
            print("ğŸ”“ Conditioner is UNFROZEN (trainable)")
            print(f"   Total conditioner params: {sum(p.numel() for p in model.conditioner.parameters()):,}")

    # 3) å‡†å¤‡ ignored_modules
    # æ³¨æ„ï¼šå³ä½¿ conditioner å‚ä¸è®­ç»ƒï¼Œæˆ‘ä»¬ä»ç„¶å°†å…¶æ”¾å…¥ ignored_modules
    # è¿™æ ·å¯ä»¥é¿å… FSDP åŒ…è£… ShapeVAE çš„å¤æ‚ç»“æ„ï¼ŒåŒæ—¶æ¢¯åº¦ä»ç„¶å¯ä»¥æ­£å¸¸æµåŠ¨
    # conditioner çš„å‚æ•°ä¼šç”± DataParallel-like æ–¹å¼å¤„ç†ï¼ˆæ¯ä¸ª rank å®Œæ•´å‰¯æœ¬ï¼‰
    ignored = [m for m in [getattr(model, "conditioner", None)] if isinstance(m, torch.nn.Module)]

    # 4) åˆ›å»º FSDPStrategyï¼ˆåˆå§‹åŒ–æ—¶å°±ä¼ å…¥ ignored_modulesï¼‰
    strategy = FSDPStrategy(
        auto_wrap_policy={Block},
        state_dict_type=fsdp_state_dict_type,  # å¯é…ç½®ï¼šfullï¼ˆæ…¢ä½†å…¼å®¹ï¼‰æˆ– shardedï¼ˆå¿«ï¼‰
        ignored_modules=ignored,
        use_orig_params=True,
        # cpu_offload=True,
    )

    # 5) åˆ›å»º Fabric å¹¶ launch
    fabric = L.Fabric(
        devices=devices,
        strategy=strategy,
        precision="bf16-mixed",
        loggers=[wandb_logger],
    )
    fabric.launch()

    # 6) ========== ç²¾åº¦ç®¡ç†ç­–ç•¥ï¼ˆç»Ÿä¸€ä¸º fp32 å‚æ•° + bf16 è®¡ç®—ï¼‰==========
    # ç›®æ ‡ï¼šè®© conditioner ä¸ä¸»ç½‘ç»œä¿æŒç›¸åŒçš„ mixed precision ç­–ç•¥
    # - å‚æ•°å­˜å‚¨ï¼šfp32ï¼ˆé«˜ç²¾åº¦ï¼Œé¿å…ç´¯ç§¯è¯¯å·®ï¼‰
    # - å‰å‘è®¡ç®—ï¼šbf16ï¼ˆè‡ªåŠ¨è½¬æ¢ï¼Œåˆ©ç”¨ Tensor Coreï¼‰
    # - æ¢¯åº¦ç´¯ç§¯ï¼šfp32ï¼ˆæ•°å€¼ç¨³å®šï¼‰
    # - ä¼˜åŒ–å™¨çŠ¶æ€ï¼šfp32ï¼ˆAdam åŠ¨é‡/æ–¹å·®ï¼‰
    if hasattr(model, "conditioner") and isinstance(model.conditioner, torch.nn.Module):
        # ç­–ç•¥1: ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        move_module_strict(model.conditioner, fabric.device)
        
        # ç­–ç•¥2: ç»Ÿä¸€è½¬æ¢ä¸º fp32ï¼ˆä¸ FSDP ä¸»ç½‘ç»œä¸€è‡´ï¼‰
        # âš ï¸ æ³¨æ„ï¼šä¸è¦è½¬ä¸º bf16ï¼é‚£ä¼šå¯¼è‡´æ¢¯åº¦ä¹Ÿæ˜¯ bf16ï¼Œç²¾åº¦ä¸è¶³
        for name, param in model.conditioner.named_parameters():
            if param.dtype != torch.float32:
                param.data = param.data.to(torch.float32)
        
        # ç­–ç•¥3: è½¬æ¢æ‰€æœ‰ buffers ä¸º fp32
        for name, buffer in model.conditioner.named_buffers():
            if buffer.dtype not in [torch.long, torch.int, torch.bool]:  # ä¿ç•™æ•´æ•°ç±»å‹
                if buffer.dtype != torch.float32:
                    buffer.data = buffer.data.to(torch.float32)
        
        print(f"âœ… Conditioner precision unified to fp32 (same as main network)")
        print(f"   Device: {fabric.device}")
        print(f"   Params dtype: {next(model.conditioner.parameters()).dtype}")
        print(f"   Training: bf16-mixed (fp32 params â†’ bf16 compute â†’ fp32 grads)")
        print(f"   Memory overhead: ~{sum(p.numel() for p in model.conditioner.parameters()) * 2 / 1024**2:.1f} MB (vs bf16)")

    # 7) ç»Ÿä¸€é-conditioner å‚æ•°ä¸º fp32ï¼Œé˜²æ­¢ FSDP æ‰å¹³åŒ–æ—¶æŠ¥ "mixed dtypes"
    def cast_non_conditioner_fp32(m: torch.nn.Module):
        cond = getattr(m, "conditioner", None)
        for sub in m.modules():
            if sub is cond:
                continue
            for p in sub.parameters(recurse=False):
                if p.dtype != torch.float32:
                    p.data = p.data.to(torch.float32)
    # cast_non_conditioner_fp32(model)

    # 8) è¿›å…¥ä¸»æµç¨‹
    main(fabric, model, train_data_dir, val_data_dir, resume)

def main(fabric, model, train_data_dir, val_data_dir, resume, **overides):
    monitor = Monitor(fabric, window_size=1, time_unit="seconds", log_iter_interval=log_iter_interval)

    if fabric.global_rank == 0:
        out_dir.mkdir(parents=True, exist_ok=True)


    # è¿™é‡Œä¸å† from_name/é‡æ–°å»ºæ¨¡äº†ï¼Œç›´æ¥ç”¨ä¼ è¿›æ¥çš„ model
    config = model.config

    train_dataloader, val_dataloader = create_dataloaders(
        batch_size=micro_batch_size,
        block_size=config.block_size,
        fabric=fabric,
        train_data_dir=train_data_dir,
        val_data_dir=val_data_dir,
        seed=42,
    )

    if val_dataloader is None:
        train_dataloader = fabric.setup_dataloaders(train_dataloader)
    else:
        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)

    fabric.seed_everything(42)

    fabric.print(f"Loading model with {config.__dict__}")
    fabric.print(f"Total parameters {num_parameters(model):,}")
    fabric.print(model)

    # ç»Ÿä¸€ç”± Fabric/FSDP æ¬åˆ°å„è‡ª rank çš„è®¾å¤‡
    model = fabric.setup(model)

    # ========== æ„å»ºä¼˜åŒ–å™¨ï¼ˆåŒºåˆ† conditioner å’Œä¸»å¹²çš„å­¦ä¹ ç‡ï¼‰ ==========
    if not freeze_conditioner and hasattr(model, "conditioner") and model.conditioner is not None:
        # è·å–åŸå§‹æ¨¡å‹ï¼ˆå¤„ç† FSDP åŒ…è£…ï¼‰
        raw_model = model.module if hasattr(model, 'module') else model
        
        # åˆ†ç¦» conditioner å‚æ•°å’Œå…¶ä»–å‚æ•°
        conditioner_params = []
        other_params = []
        conditioner_param_ids = set(id(p) for p in raw_model.conditioner.parameters())
        
        for name, param in raw_model.named_parameters():
            if param.requires_grad:
                if id(param) in conditioner_param_ids:
                    conditioner_params.append(param)
                else:
                    other_params.append(param)
        
        # ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡çš„å‚æ•°ç»„
        param_groups = [
            {"params": other_params, "lr": learning_rate},
            {"params": conditioner_params, "lr": learning_rate * conditioner_lr_scale, "name": "conditioner"},
        ]
        
        fabric.print(f"ğŸ”§ Optimizer setup with separate learning rates:")
        fabric.print(f"   - Main model params: {len(other_params)}, lr={learning_rate}")
        fabric.print(f"   - Conditioner params: {len(conditioner_params)}, lr={learning_rate * conditioner_lr_scale}")
        
        optimizer = torch.optim.AdamW(
            param_groups, weight_decay=weight_decay, betas=(beta1, beta2), fused=True
        )
    else:
        # åŸæœ‰é€»è¾‘ï¼šæ‰€æœ‰å‚æ•°ä½¿ç”¨ç›¸åŒå­¦ä¹ ç‡
        optimizer = torch.optim.AdamW(
            model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), fused=True
        )
    
    optimizer = fabric.setup_optimizers(optimizer)

    state = {"model": model, "optimizer": optimizer, "hparams": hparams, "iter_num": 0, "step_count": 0, "epoch": 0}

    if resume is True:
        resume = sorted(out_dir.glob("*.pth"))[-1]
    if resume:
        fabric.print(f"\n{'='*60}")
        fabric.print(f"ğŸ“¥ Resuming training from {resume}")
        fabric.print(f"   FSDP state_dict_type: {fsdp_state_dict_type}")
        fabric.print(f"   Current freeze_conditioner: {freeze_conditioner}")
        fabric.print(f"{'='*60}")
        
        # è®°å½•åŠ è½½æ—¶é—´
        t0 = time.perf_counter()
        resume_path = Path(resume) if not isinstance(resume, Path) else resume
        
        # ========== æ™ºèƒ½åŠ è½½ï¼šå¤„ç† optimizer å‚æ•°ç»„ä¸åŒ¹é… ==========
        skip_optimizer = False
        if fabric.global_rank == 0:
            try:
                ckpt_preview = torch.load(resume_path, map_location='cpu', weights_only=False)
                if 'optimizer' in ckpt_preview:
                    saved_param_groups = len(ckpt_preview['optimizer'].get('param_groups', []))
                    current_param_groups = len(optimizer.param_groups)
                    if saved_param_groups != current_param_groups:
                        fabric.print(f"âš ï¸  Optimizer param groups mismatch: saved={saved_param_groups}, current={current_param_groups}")
                        fabric.print(f"   This is normal when switching freeze_conditioner setting.")
                        fabric.print(f"   Will skip optimizer state and re-initialize.")
                        skip_optimizer = True
                del ckpt_preview
            except Exception as e:
                fabric.print(f"âš ï¸  Could not preview checkpoint: {e}")
        
        # å¹¿æ’­ skip_optimizer ç»™æ‰€æœ‰ rank
        if fabric.world_size > 1:
            skip_tensor = torch.tensor([skip_optimizer], dtype=torch.int32, device=fabric.device)
            torch.distributed.broadcast(skip_tensor, src=0)
            skip_optimizer = bool(skip_tensor.item())
        
        if skip_optimizer:
            # åªåŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸åŠ è½½ optimizer
            state_model_only = {"model": model, "iter_num": 0, "step_count": 0, "epoch": 0}
            load_checkpoint_with_conditioner(fabric, resume_path, state_model_only, model)
            
            # æ¢å¤è®­ç»ƒçŠ¶æ€
            state["iter_num"] = state_model_only.get("iter_num", 0)
            state["step_count"] = state_model_only.get("step_count", 0)
            state["epoch"] = state_model_only.get("epoch", 0)
            
            fabric.print(f"âœ… Model weights loaded, optimizer re-initialized")
            fabric.print(f"   Resumed from iter={state['iter_num']}, step={state['step_count']}, epoch={state['epoch']}")
        else:
            # æ­£å¸¸åŠ è½½ï¼ˆåŒ…å« optimizerï¼‰
            load_checkpoint_with_conditioner(fabric, resume_path, state, model)
            fabric.print(f"âœ… Full checkpoint loaded (model + optimizer + conditioner)")
        
        t1 = time.perf_counter()
        fabric.print(f"â±ï¸  Total resume time: {t1 - t0:.2f}s")
        fabric.print(f"{'='*60}\n")
        
        fabric.barrier()  # ç¡®ä¿æ‰€æœ‰ rank åŒæ­¥

    train_time = time.perf_counter()
    train(fabric, state, train_dataloader, val_dataloader, monitor, resume)
    fabric.print(f"Training time: {(time.perf_counter()-train_time):.2f}s")
    if fabric.device.type == "cuda":
        fabric.print(f"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB")

def train(fabric, state, train_dataloader, val_dataloader, monitor, resume):
    model = state["model"]
    optimizer = state["optimizer"]

    if val_dataloader is not None:
        validate(fabric, model, val_dataloader)  # sanity check

    # ------- ä»ç„¶åœ¨ meta ä¸Šä¼° FLOPsï¼Œä½†å…³é—­ conditioner -------
    with torch.device("meta"):
        meta_model = GPT(model.config, build_conditioner=False)
        estimated_flops = estimate_flops(meta_model) * micro_batch_size
        fabric.print(f"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}")
        x = torch.randint(0, 1, (micro_batch_size, model.config.block_size))
        del meta_model, x
    # ------------------------------------------------------

    total_lengths = 0
    total_t0 = time.perf_counter()

    if fabric.device.type == "xla":
        import torch_xla.core.xla_model as xm
        xm.mark_step()

    # ä½¿ç”¨ epoch è®¡æ•°çš„è°ƒåº¦å‚æ•°
    steps_per_epoch = len(train_dataloader)
    total_iters = steps_per_epoch * max(1, num_epochs)
    max_iters = total_iters
    warmup_iters = max(1, int(0.01 * total_iters)) if decay_lr else 0
    initial_iter = state["iter_num"]

    loss_func = FusedCrossEntropyLoss()
    
    # ========== CD Loss åŠ æƒç›¸å…³ï¼šEMA ç»Ÿè®¡å™¨ ==========
    # ç”¨äºè·Ÿè¸ª CD çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œåšç¨³å¥å½’ä¸€åŒ–
    cd_ema_stats = EMAStats(momentum=0.99, eps=1e-8)
    
    # è®­ç»ƒå‰è¯Šæ–­ï¼šæ£€æŸ¥ condition æµç¨‹
    if fabric.global_rank == 0 and state["iter_num"] == 0:
        fabric.print("\n" + "="*60)
        fabric.print("ğŸ” Condition Pipeline Diagnostic Check")
        fabric.print("="*60)
        raw_model = model.module if hasattr(model, 'module') else model
        if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
            is_frozen = not any(p.requires_grad for p in raw_model.conditioner.parameters())
            fabric.print("âœ… Conditioner found (ShapeVAE from config.yaml)")
            fabric.print(f"   - Status: {'ğŸ”’ FROZEN' if is_frozen else 'ğŸ”“ TRAINABLE'}")
            fabric.print(f"   - Mode: {'eval' if not raw_model.conditioner.training else 'train'}")
            fabric.print(f"   - Dtype: {next(raw_model.conditioner.parameters()).dtype}")
            fabric.print(f"   - Device: {next(raw_model.conditioner.parameters()).device}")
            
            if not is_frozen:
                cond_params = sum(p.numel() for p in raw_model.conditioner.parameters() if p.requires_grad)
                fabric.print(f"   - Trainable params: {cond_params:,}")
                fabric.print(f"   - Learning rate scale: {conditioner_lr_scale}x")
            
            # éªŒè¯ ShapeVAE çš„ç¼–è§£ç æµç¨‹
            fabric.print(f"\nğŸ“‹ ShapeVAE Architecture:")
            fabric.print(f"   - Latent shape: {raw_model.conditioner.latent_shape}")
            fabric.print(f"   - Encoder: PointCrossAttentionEncoder")
            fabric.print(f"   - Decoder: post_kl + Transformer")
            fabric.print(f"   - Expected workflow: pc â†’ encode() â†’ latent_codes â†’ decode() â†’ features")
            
            # æ£€æŸ¥å„ç»„ä»¶çš„ dtype
            fabric.print(f"\nğŸ”§ Component Dtypes:")
            if hasattr(raw_model.conditioner, 'encoder'):
                fabric.print(f"   - Encoder: {next(raw_model.conditioner.encoder.parameters()).dtype}")
            if hasattr(raw_model.conditioner, 'pre_kl'):
                fabric.print(f"   - Pre-KL: {raw_model.conditioner.pre_kl.weight.dtype}")
            if hasattr(raw_model.conditioner, 'post_kl'):
                fabric.print(f"   - Post-KL: {raw_model.conditioner.post_kl.weight.dtype}")
            if hasattr(raw_model.conditioner, 'transformer'):
                fabric.print(f"   - Transformer: {next(raw_model.conditioner.transformer.parameters()).dtype}")
            
            if hasattr(raw_model, 'linear'):
                fabric.print(f"\nâœ… Linear projection layer found")
                fabric.print(f"   - input_dim: {raw_model.linear.in_features}")
                fabric.print(f"   - output_dim: {raw_model.linear.out_features}")
                fabric.print(f"   - dtype: {raw_model.linear.weight.dtype}")
                fabric.print(f"   - trainable: {raw_model.linear.weight.requires_grad}")
            
            # ç»Ÿè®¡ cross-attention å±‚æ•°é‡
            cross_attn_count = sum(1 for name, _ in raw_model.named_modules() if 'cross_attn' in name)
            fabric.print(f"\nâœ… Found {cross_attn_count} CrossAttention layers")
            
            # æ ¹æ®é™é‡‡æ ·é…ç½®è®¡ç®—æœŸæœ›çš„ token æ•°
            num_latents = raw_model.conditioner.latent_shape[0]
            if hasattr(raw_model, 'condition_downsample_factor'):
                expected_tokens = num_latents // raw_model.condition_downsample_factor
                fabric.print(f"   Expected condition tokens: {expected_tokens} ({num_latents} â†’ downsample by {raw_model.condition_downsample_factor}x)")
                if hasattr(raw_model, 'condition_downsample') and raw_model.condition_downsample is not None:
                    fabric.print(f"   Downsample method: learnable (MLP)")
                else:
                    fabric.print(f"   Downsample method: average pooling")
            else:
                fabric.print(f"   Expected condition tokens: {num_latents} (no downsampling)")
        else:
            fabric.print("âš ï¸  WARNING: No conditioner found!")
        fabric.print("="*60 + "\n")
    
    # ========== æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯ ==========
    
    
    # ä»¥ epoch ä¸ºå•ä½è®­ç»ƒ
    for epoch in range(state.get("epoch", 0), max(1, num_epochs)):
        try:
            sampler = getattr(train_dataloader, 'sampler', None)
            if hasattr(sampler, 'set_epoch'):
                sampler.set_epoch(epoch)
        except Exception:
            pass
        idddx = 0
        for train_data in train_dataloader:
            idddx += 1
            if state["iter_num"] >= max_iters:
                break

            # determine and set the learning rate for this iteration
            lr = get_lr(state["iter_num"], warmup_iters, max_iters) if decay_lr else learning_rate
            for param_group in optimizer.param_groups:
                # å¦‚æœæ˜¯ conditioner å‚æ•°ç»„ï¼Œä½¿ç”¨ç¼©æ”¾çš„å­¦ä¹ ç‡
                if param_group.get("name") == "conditioner":
                    param_group["lr"] = lr * conditioner_lr_scale
                else:
                    param_group["lr"] = lr

            iter_t0 = time.perf_counter()

            # å¤„ç†ç‚¹äº‘ pcï¼ˆå¯é€‰ï¼Œä¸å‚ä¸ GPT æŸå¤±ï¼Œä½†ç§»åŠ¨åˆ°å½“å‰è®¾å¤‡ï¼‰
            pc_list = None
            if isinstance(train_data, dict):
                pc_list = train_data.get('pc', None)
                if pc_list is None:
                    pc_list = train_data.get('pc_normal', None)
            
            # æ£€æŸ¥ pc_list æ˜¯å¦ä¸º None æˆ–ç©ºåˆ—è¡¨ï¼Œé¿å… torch.stack å´©æºƒ
            if pc_list is None or len(pc_list) == 0:
                fabric.print(f"Warning: pc_list is None or empty at iter {state['iter_num']}. Skipping this batch.")
                state["iter_num"] += 1
                continue
            
            pc = torch.stack(pc_list, dim=0).to(fabric.device)  # ç¡®ä¿ä¸æ¨¡å‹åŒè®¾å¤‡
            
            # æ£€æŸ¥ pc çš„å½¢çŠ¶
            if pc.dim() != 3 or pc.shape[1] != 81920 or pc.shape[2] != 7:
                fabric.print(f"Warning: pc has unexpected shape {pc.shape} at iter {state['iter_num']}. Skipping this batch.")
                state["iter_num"] += 1
                continue
            # å¤„ç† tokensï¼špadding åˆ° batch å†…æœ€å¤§é•¿åº¦
            # ä½¿ç”¨ EOS_TOKEN_ID (4737) å¡«å……ï¼Œé€šè¿‡ä½ç½® mask æ¥å±è”½ paddingï¼ˆä¿æŒ vocab ä¸å˜ï¼‰
            token_lists = train_data['token_list_0'] if isinstance(train_data, dict) and 'token_list_0' in train_data else None
            maxL = 0
            minL = 9999999
            sample_lengths = []  # è®°å½•æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦ï¼ˆæˆªæ–­åï¼‰
            if token_lists is not None and len(token_lists) > 0:
                token_tensors = []
                for t in token_lists:
                    t_tensor = t if torch.is_tensor(t) else torch.tensor(t, dtype=torch.long)
                    token_tensors.append(t_tensor)
                    maxL = max(maxL, t_tensor.numel())
                    minL = min(minL, t_tensor.numel())
                
                # ä½¿ç”¨ EOS_TOKEN_ID ä½œä¸º padding å¡«å……ï¼ˆä¿æŒ vocab ä¸å˜ï¼Œå…¼å®¹æ—§ ckptï¼‰
                pad_id = EOS_TOKEN_ID
                max_len = 9001
                padded_token_tensors = []
                for t in token_tensors:
                    orig_len = t.numel()
                    # æœ‰æ•ˆé•¿åº¦ = min(åŸå§‹é•¿åº¦, max_len)ï¼Œæˆªæ–­æ—¶åŒæ­¥æ›´æ–°
                    effective_len = min(orig_len, max_len)
                    sample_lengths.append(effective_len)
                    
                    if orig_len < max_len:
                        # éœ€è¦ padding
                        pad_len = max_len - orig_len
                        pad = torch.full((pad_len,), pad_id, dtype=torch.long, device=t.device)
                        padded_t = torch.cat([t, pad], dim=0)
                    elif orig_len > max_len:
                        # éœ€è¦æˆªæ–­
                        padded_t = t[:max_len]
                    else:
                        # åˆšå¥½ç­‰äº max_len
                        padded_t = t
                    padded_token_tensors.append(padded_t)
                
                merged_token_tensor = torch.stack(padded_token_tensors, dim=0).to(fabric.device)
                # å°†é•¿åº¦ä¿¡æ¯ä¹Ÿè½¬ä¸º tensor
                sample_lengths = torch.tensor(sample_lengths, device=fabric.device, dtype=torch.long)
            else:
                merged_token_tensor = None
                sample_lengths = None

            # æ£€æŸ¥ merged_token_tensor æ˜¯å¦ä¸º None
            if merged_token_tensor is None:
                fabric.print(f"Warning: merged_token_tensor is None at iter {state['iter_num']}. "
                           f"train_data keys: {list(train_data.keys()) if isinstance(train_data, dict) else 'not a dict'}. "
                           f"Skipping this batch.")
                state["iter_num"] += 1
                continue

            input_token = merged_token_tensor[:, :-1].contiguous()
            target_token = merged_token_tensor[:, 1:].contiguous()
            batch_size, seq_len = target_token.shape
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆ target é•¿åº¦ï¼ˆåŸå§‹é•¿åº¦ - 1ï¼Œå› ä¸º target æ˜¯å³ç§»ä¸€ä½ï¼‰
            valid_lens = (sample_lengths - 1).clamp(min=1, max=seq_len)  # (B,)

            # print(f"input_token: {input_token.shape}, target_token: {target_token.shape}, pc: {pc.shape}, batch_size: {batch_size}, seq_len: {seq_len}, gradient_accumulation_steps: {gradient_accumulation_steps}")
            # print(f"pc[0]: {pc[0][:5]}, pc[1]: {pc[1][:5]}, pc[2]: {pc[2][:5]}, pc[3]: {pc[3][:5]}")
            is_accumulating = (state["iter_num"] + 1) % gradient_accumulation_steps != 0
            
            # ç›‘æ§ condition embeddings ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯ 500 æ­¥ï¼‰
            monitor_condition = (state["iter_num"] % 500 == 0) and fabric.global_rank == 0
            
            with fabric.no_backward_sync(model, enabled=is_accumulating):
                logits = model(input_token, pc=pc, window_size=9000).logits
                
                # ========== è®¡ç®— per-sample CE Lossï¼ˆç”¨äº CD åŠ æƒï¼‰==========
                # ä½¿ç”¨ F.cross_entropy è·å– per-token loss
                per_token_loss = torch.nn.functional.cross_entropy(
                    logits.view(-1, logits.size(-1)),  # (batch_size * seq_len, vocab_size)
                    target_token.view(-1),              # (batch_size * seq_len,)
                    reduction='none'
                ).view(batch_size, seq_len)  # (batch_size, seq_len)
                
                # ========== ä½¿ç”¨ä½ç½® maskï¼ˆä¸ä¾èµ– token idï¼Œé¿å…è¯¯ä¼¤çœŸå® EOSï¼‰==========
                # åˆ›å»ºä½ç½® mask: True è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼ˆpos < valid_lenï¼‰ï¼ŒFalse è¡¨ç¤º padding
                pos = torch.arange(seq_len, device=fabric.device).unsqueeze(0)  # (1, T)
                pad_mask = (pos < valid_lens.unsqueeze(1)).float()  # (B, T)
                
                # è®¡ç®— per-sample lossï¼ˆåªå¯¹æœ‰æ•ˆä½ç½®æ±‚å¹³å‡ï¼‰
                masked_loss = per_token_loss * pad_mask  # (batch_size, seq_len)
                per_sample_loss = masked_loss.sum(dim=1) / valid_lens.float().clamp(min=1.0)  # (batch_size,)
                
                # batch å¹³å‡ lossï¼ˆç”¨äºç›‘æ§å’ŒåŸå§‹ backwardï¼Œåé¢ä¼šè¢«åŠ æƒç‰ˆæœ¬æ›¿æ¢ï¼‰
                loss = per_sample_loss.mean()
                
                # ========== è®¡ç®— Chamfer Distanceï¼ˆCD Lossï¼‰ç”¨äºåŠ æƒ CE Loss ==========
                # æ‰€æœ‰ rank éƒ½éœ€è¦è®¡ç®—ï¼ˆå› ä¸ºè¦ç”¨äº per-sample æƒé‡ï¼‰
                cd_loss_value = None  # batch å¹³å‡å€¼ï¼ˆç”¨äºç›‘æ§ï¼‰
                cd_per_sample = []    # per-sample CD å€¼ï¼ˆç”¨äºåŠ æƒï¼‰
                cd_z_scores = None    # å½’ä¸€åŒ–åçš„ z å€¼
                
                # åœ¨å¼€å…³å¼€å¯ä¸”éç´¯ç§¯æ­¥éª¤æ—¶è®¡ç®—
                if compute_cd_loss and not is_accumulating:
                    with torch.no_grad():
                        try:
                            # è·å–é¢„æµ‹çš„ tokenï¼ˆè´ªå©ªè§£ç ï¼‰
                            pred_tokens = torch.argmax(logits, dim=-1)  # (batch_size, seq_len)
                            
                            # æ˜¯å¦æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼ˆæ ¹æ®é…ç½®çš„é—´éš”ï¼Œåªåœ¨ rank 0 æ‰“å°ï¼‰
                            debug_mode = (state["iter_num"] % cd_loss_log_interval == 0) and fabric.global_rank == 0
                            
                            # æ‰¹é‡è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„ CD
                            cd_losses = []
                            batch_size_local = pred_tokens.shape[0]
                            
                            if debug_mode:
                                fabric.print(f"\n[Chamfer Distance @ iter {state['iter_num']}]")
                                fabric.print(f"  Batch size: {batch_size_local}")
                            
                            for i in range(batch_size_local):
                                # æŒ‰çœŸå®é•¿åº¦æˆªæ–­ï¼ˆä¸å†ä¾èµ– token id è¿‡æ»¤ paddingï¼‰
                                L = int(valid_lens[i].item())  # æœ‰æ•ˆ target é•¿åº¦
                                pred_seq = pred_tokens[i, :L]
                                gt_seq = target_token[i, :L]
                                
                                # è§£ç ä¸º mesh å¹¶é‡‡æ ·ç‚¹ï¼ˆ1024 ä¸ªç‚¹ï¼‰
                                pred_points = tokens_to_mesh_with_sampling(pred_seq, num_samples=1024)
                                gt_points = tokens_to_mesh_with_sampling(gt_seq, num_samples=1024)
                                
                                # è®¡ç®— CDï¼ˆä¼ å…¥ device é¿å…å¤šå¡ç¯å¢ƒä¸‹è®¾å¤‡ä¸åŒ¹é…ï¼‰
                                if pred_points is not None and gt_points is not None:
                                    cd = compute_chamfer_distance_fast(pred_points, gt_points, device=fabric.device)
                                    # æ£€æŸ¥ CD æ˜¯å¦æœ‰æ•ˆï¼ˆé inf/nanï¼‰
                                    if cd == float('inf') or cd != cd:  # cd != cd æ£€æµ‹ NaN
                                        cd_losses.append(10.0)  # ä½¿ç”¨ placeholder
                                        if debug_mode:
                                            fabric.print(f"  Sample {i}: âš ï¸ CD computation returned inf/nan, using placeholder=10.0")
                                    else:
                                        cd_losses.append(cd)
                                        if debug_mode:
                                            fabric.print(f"  Sample {i}: CD={cd:.6f} (len={L})")
                                else:
                                    # è§£ç å¤±è´¥ï¼Œä½¿ç”¨ placeholderï¼ˆä¼šè¢« EMA ç»Ÿè®¡æ„ŸçŸ¥ï¼‰
                                    cd_losses.append(10.0)
                                    if debug_mode:
                                        fabric.print(f"  Sample {i}: âŒ Decode failed (len={L}), using placeholder=10.0")
                            
                            # ä¿å­˜ per-sample CD
                            cd_per_sample = cd_losses.copy()
                            
                            # æ›´æ–° EMA ç»Ÿè®¡é‡ï¼ˆåªåœ¨ rank 0 æ›´æ–°ï¼Œç„¶ååŒæ­¥ç»™å…¶ä»– rankï¼‰
                            # è¿™ç¡®ä¿æ‰€æœ‰ rank ä½¿ç”¨ç›¸åŒçš„ mean/varï¼Œæƒé‡åˆ†å¸ƒä¸€è‡´
                            valid_cds = [cd for cd in cd_losses if cd < 10.0]
                            if fabric.global_rank == 0 and len(valid_cds) > 0:
                                cd_ema_stats.update(valid_cds)
                            
                            # åŒæ­¥ EMA ç»Ÿè®¡é‡åˆ°æ‰€æœ‰ rank
                            cd_ema_stats.sync_across_ranks(fabric)
                            
                            # å½’ä¸€åŒ–ä¸º z-score
                            cd_z_scores = cd_ema_stats.normalize(cd_losses)
                            
                            # è®¡ç®— batch å¹³å‡ CDï¼ˆç”¨äºç›‘æ§ï¼‰
                            cd_loss_value = np.mean(cd_losses)
                            
                            if debug_mode:
                                ema_mean, ema_std = cd_ema_stats.get_stats()
                                fabric.print(f"\n  Summary:")
                                fabric.print(f"    Valid samples: {len(valid_cds)}/{batch_size_local}")
                                fabric.print(f"    CD Loss (avg): {cd_loss_value:.6f}")
                                fabric.print(f"    EMA stats: mean={ema_mean:.6f}, std={ema_std:.6f}")
                                fabric.print(f"    Z-scores: {cd_z_scores}")
                        
                        except Exception as e:
                            if fabric.global_rank == 0 and state["iter_num"] % cd_loss_log_interval == 0:
                                fabric.print(f"\n[Chamfer Distance @ iter {state['iter_num']}]")
                                fabric.print(f"  âš ï¸  CD computation failed: {e}")
                                import traceback
                                traceback.print_exc()
                            cd_loss_value = 10.0
                            cd_per_sample = [10.0] * batch_size
                            cd_z_scores = np.zeros(batch_size, dtype=np.float32)
                
                
                # ç›‘æ§ condition ä¿¡æ¯
                if monitor_condition:
                    with torch.no_grad():
                        # è·å– condition embeddingsï¼ˆé€šè¿‡å®Œæ•´çš„ encode + decode æµç¨‹ï¼‰
                        try:
                            # è®¿é—® conditionerï¼ˆéœ€è¦å¤„ç† FSDP åŒ…è£…ï¼‰
                            raw_model = model.module if hasattr(model, 'module') else model
                            if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
                                # Stage 1: ç¼–ç ä¸º latent codes
                                latent_codes = raw_model.conditioner.encode(pc, sample_posterior=False)
                                # latent_codes: (bs, num_latents, embed_dim)
                                
                                # Stage 2: è§£ç ä¸ºç‰¹å¾
                                cond_embeds = raw_model.conditioner.decode(latent_codes)
                                # cond_embeds: (bs, num_latents, width)
                                
                                # åº”ç”¨é™é‡‡æ ·ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                                if hasattr(raw_model, 'condition_downsample_factor') and raw_model.condition_downsample_factor > 1:
                                    from einops import rearrange
                                    factor = raw_model.condition_downsample_factor
                                    if hasattr(raw_model, 'condition_downsample') and raw_model.condition_downsample is not None:
                                        # å¯å­¦ä¹ é™é‡‡æ · - ç¡®ä¿ dtype åŒ¹é…
                                        target_dtype = next(raw_model.condition_downsample.parameters()).dtype
                                        cond_embeds_downsampled = rearrange(cond_embeds, 'b (n f) d -> b n (f d)', f=factor)
                                        cond_embeds_downsampled = cond_embeds_downsampled.to(target_dtype)
                                        cond_embeds_downsampled = raw_model.condition_downsample(cond_embeds_downsampled)
                                    else:
                                        # å¹³å‡æ± åŒ–
                                        cond_embeds_downsampled = rearrange(cond_embeds, 'b (n f) d -> b n f d', f=factor)
                                        cond_embeds_downsampled = cond_embeds_downsampled.mean(dim=2)
                                else:
                                    cond_embeds_downsampled = cond_embeds
                                
                                # Project åˆ° model dimensionï¼ˆç¡®ä¿ dtype å®Œå…¨åŒ¹é…ï¼‰
                                linear_dtype = raw_model.linear.weight.dtype
                                linear_device = raw_model.linear.weight.device
                                cond_embeds_downsampled = cond_embeds_downsampled.to(dtype=linear_dtype, device=linear_device)
                                cond_embeds_proj = raw_model.linear(cond_embeds_downsampled)
                                
                                condition_stats = {
                                    "condition/latent_codes_mean": latent_codes.float().mean().item(),
                                    "condition/latent_codes_std": latent_codes.float().std().item(),
                                    "condition/decoded_mean": cond_embeds.float().mean().item(),
                                    "condition/decoded_std": cond_embeds.float().std().item(),
                                    "condition/proj_mean": cond_embeds_proj.float().mean().item(),
                                    "condition/proj_std": cond_embeds_proj.float().std().item(),
                                    "condition/num_tokens": cond_embeds_proj.shape[1],
                                }
                                
                                fabric.print(f"\n[Condition Stats @ iter {state['iter_num']}]")
                                fabric.print(f"  Latent codes: mean={condition_stats['condition/latent_codes_mean']:.4f}, "
                                           f"std={condition_stats['condition/latent_codes_std']:.4f}")
                                fabric.print(f"  Decoded features: mean={condition_stats['condition/decoded_mean']:.4f}, "
                                           f"std={condition_stats['condition/decoded_std']:.4f}")
                                fabric.print(f"  Projected: mean={condition_stats['condition/proj_mean']:.4f}, "
                                           f"std={condition_stats['condition/proj_std']:.4f}")
                                fabric.print(f"  Num context tokens: {condition_stats['condition/num_tokens']}")
                                
                                fabric.log_dict(condition_stats, state["step_count"])
                        except Exception as e:
                            fabric.print(f"Warning: Failed to monitor condition stats: {e}")
                
                # ========== åº”ç”¨ CD æƒé‡åˆ° CE Loss ==========
                # ç”¨äº backward çš„ lossï¼ˆå¯èƒ½æ˜¯åŠ æƒåçš„ï¼‰
                weighted_loss = loss  # é»˜è®¤ä½¿ç”¨åŸå§‹ loss
                cd_weights = None     # ç”¨äºæ—¥å¿—
                
                if compute_cd_loss and cd_z_scores is not None and not is_accumulating:
                    try:
                        # æ£€æŸ¥ z_scores é•¿åº¦æ˜¯å¦ä¸ batch_size åŒ¹é…
                        if len(cd_z_scores) != batch_size:
                            if fabric.global_rank == 0:
                                fabric.print(f"  âš ï¸  CD z_scores length mismatch: {len(cd_z_scores)} vs batch_size={batch_size}, skipping weighting")
                        else:
                            # è®¡ç®—æƒé‡ï¼šz > 0 (CDå·®) â†’ æƒé‡å¤§ï¼Œz < 0 (CDå¥½) â†’ æƒé‡å°
                            cd_weights = compute_cd_weights(cd_z_scores, w_min=0.5, w_max=2.0, k=1.0)
                            cd_weights_tensor = torch.from_numpy(cd_weights).float().to(per_sample_loss.device)
                            
                            # æ£€æŸ¥æƒé‡æ˜¯å¦åŒ…å« NaN æˆ– Inf
                            if torch.isnan(cd_weights_tensor).any() or torch.isinf(cd_weights_tensor).any():
                                if fabric.global_rank == 0:
                                    fabric.print(f"  âš ï¸  CD weights contain NaN/Inf, skipping weighting")
                                cd_weights = None
                            else:
                                # åŠ æƒ per-sample loss å¹¶æ±‚å¹³å‡
                                weighted_loss = (cd_weights_tensor * per_sample_loss).mean()
                                
                                # æ£€æŸ¥åŠ æƒåçš„ loss æ˜¯å¦æœ‰æ•ˆ
                                if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                                    if fabric.global_rank == 0:
                                        fabric.print(f"  âš ï¸  Weighted loss is NaN/Inf, falling back to original loss")
                                    weighted_loss = loss
                                    cd_weights = None
                                else:
                                    # æ‰“å°æƒé‡ä¿¡æ¯ï¼ˆæ¯ cd_loss_log_interval æ­¥ï¼Œåªåœ¨ rank 0ï¼‰
                                    if fabric.global_rank == 0 and state["iter_num"] % cd_loss_log_interval == 0:
                                        fabric.print(f"  CD Weights: min={cd_weights.min():.4f}, max={cd_weights.max():.4f}, "
                                                   f"mean={cd_weights.mean():.4f}")
                                        fabric.print(f"  Loss: original={loss.item():.4f}, weighted={weighted_loss.item():.4f}")
                    except Exception as e:
                        if fabric.global_rank == 0:
                            fabric.print(f"  âš ï¸  CD weighting failed: {e}, using original loss")
                        weighted_loss = loss
                        cd_weights = None
                
                fabric.backward(weighted_loss / gradient_accumulation_steps)

            if not is_accumulating:
                # ========== åŒæ­¥ conditioner æ¢¯åº¦ï¼ˆå¦‚æœè§£é”è®­ç»ƒï¼‰ ==========
                # ç”±äº conditioner åœ¨ FSDP çš„ ignored_modules ä¸­ï¼Œå¤š GPU æ—¶æ¢¯åº¦ä¸ä¼šè‡ªåŠ¨åŒæ­¥
                # éœ€è¦æ‰‹åŠ¨è¿›è¡Œ all-reduce
                if not freeze_conditioner and fabric.world_size > 1:
                    raw_model = model.module if hasattr(model, 'module') else model
                    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
                        for param in raw_model.conditioner.parameters():
                            if param.grad is not None:
                                # all-reduce æ¢¯åº¦å¹¶å–å¹³å‡
                                torch.distributed.all_reduce(param.grad, op=torch.distributed.ReduceOp.SUM)
                                param.grad.div_(fabric.world_size)
                
                fabric.clip_gradients(model, optimizer, max_norm=grad_clip)
                
                # ç›‘æ§ condition ç›¸å…³å±‚çš„æ¢¯åº¦ï¼ˆæ¯ 100 æ­¥ï¼‰
                if state["step_count"] % 100 == 0 and fabric.global_rank == 0:
                    condition_grad_stats = {}
                    cross_attn_grads = []
                    linear_grads = []
                    
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            grad_norm = param.grad.norm().item()
                            # ç›‘æ§ cross-attention å±‚
                            if 'cross_attn' in name:
                                # åªè®°å½•æƒé‡å±‚çš„æ¢¯åº¦ï¼Œè·³è¿‡å…·ä½“å‚æ•°åä»¥ä¿æŒç®€æ´
                                if 'weight' in name:
                                    cross_attn_grads.append(grad_norm)
                                    layer_type = name.split('.')[-2]  # q_proj, kv_proj, out_proj
                                    condition_grad_stats[f"grad/cross_attn_{layer_type}"] = grad_norm
                            # ç›‘æ§ condition projection å±‚
                            elif name.endswith('linear.weight') or name.endswith('linear.bias'):
                                linear_grads.append(grad_norm)
                                condition_grad_stats[f"grad/{name.split('.')[-1]}"] = grad_norm
                    
                    if condition_grad_stats:
                        # è®¡ç®—ç»Ÿè®¡é‡
                        if cross_attn_grads:
                            condition_grad_stats["grad/cross_attn_mean"] = sum(cross_attn_grads) / len(cross_attn_grads)
                            condition_grad_stats["grad/cross_attn_max"] = max(cross_attn_grads)
                        if linear_grads:
                            condition_grad_stats["grad/linear_mean"] = sum(linear_grads) / len(linear_grads)
                        
                        fabric.print(f"\n[Condition Gradient Monitor @ step {state['step_count']}]")
                        if cross_attn_grads:
                            fabric.print(f"  CrossAttention: mean={condition_grad_stats['grad/cross_attn_mean']:.6f}, "
                                       f"max={condition_grad_stats['grad/cross_attn_max']:.6f}, "
                                       f"count={len(cross_attn_grads)}")
                        if linear_grads:
                            fabric.print(f"  Linear projection: mean={condition_grad_stats['grad/linear_mean']:.6f}")
                        
                        # è®°å½•åˆ° wandb
                        fabric.log_dict(condition_grad_stats, state["step_count"])
                
                optimizer.step()
                optimizer.zero_grad()
                state["step_count"] += 1
            elif fabric.device.type == "xla":
                import torch_xla.core.xla_model as xm
                xm.mark_step()
            state["iter_num"] += 1


            total_lengths += input_token.size(1)
            t1 = time.perf_counter()
            elapsed_iters = max(1, state['iter_num'] - initial_iter)
            remaining_hours = (t1 - total_t0) / elapsed_iters * max(0, (max_iters - state['iter_num'])) / 3600
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆåŒ…å« CD loss å’Œæƒé‡ä¿¡æ¯ï¼Œå¦‚æœå¯ç”¨ï¼‰
            cd_info = ""
            weight_info = ""
            if compute_cd_loss and cd_loss_value is not None:
                cd_info = f", CD: {cd_loss_value:.4f}" if cd_loss_value != float('inf') else ", CD: inf"
                if cd_weights is not None:
                    weight_info = f", w: [{cd_weights.min():.2f}-{cd_weights.max():.2f}]"
            
            # ä½¿ç”¨åŠ æƒåçš„ loss è¿›è¡Œæ˜¾ç¤ºï¼ˆå¦‚æœæœ‰åŠ æƒï¼‰
            display_loss = weighted_loss.item() if (compute_cd_loss and cd_weights is not None) else loss.item()
            
            fabric.print(
                f"iter {state['iter_num']} step {state['step_count']}: loss {display_loss:.4f}{cd_info}{weight_info}, "
                f"iter: {idddx}/{len(train_dataloader)} , epoch: {state['epoch']}, gap: {maxL-minL}, lr: {lr}, iter time:"
                f" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}"
                f" remaining time: {remaining_hours:.2f} hours. "
                f" or {remaining_hours / 24:.2f} days. "
            )

            monitor.on_train_batch_end(
                state["iter_num"] * micro_batch_size,
                t1 - total_t0,
                fabric.world_size,
                state["step_count"],
                flops_per_batch=estimated_flops,
                lengths=total_lengths,
                train_loss = display_loss,
                lr = lr,
                FWLoss = 0.0,
                cd_loss = cd_loss_value if (compute_cd_loss and cd_loss_value is not None and cd_loss_value != float('inf')) else None,
                # CD åŠ æƒç›¸å…³ï¼šåˆ†åˆ«è®°å½•åŸå§‹å’ŒåŠ æƒåçš„ loss
                original_loss = loss.item() if compute_cd_loss else None,
                weighted_loss = weighted_loss.item() if (compute_cd_loss and cd_weights is not None) else None,
                cd_weight_mean = float(cd_weights.mean()) if cd_weights is not None else None,
            )

            if val_dataloader is not None and not is_accumulating and state["step_count"] % eval_step_interval == 0:
                t0 = time.perf_counter()
                val_loss = validate(fabric, model, val_dataloader)
                t1 = time.perf_counter() - t0
                monitor.eval_end(t1)
                for i in range(num_extrapol):
                    fabric.print(f"step {state['iter_num']}: val loss {val_loss[i]:.4f}, val time: {t1 * 1000:.2f}ms")
                    fabric.log_dict({"metric/val_loss@"+str(i+1)+"x": val_loss[i].item(), "total_tokens": model.config.block_size * (state["iter_num"] + 1) * micro_batch_size * fabric.world_size}, state["step_count"])
                    fabric.log_dict({"metric/val_ppl@"+str(i+1)+"x": math.exp(val_loss[i].item()), "total_tokens": model.config.block_size * (state["iter_num"] + 1) * micro_batch_size * fabric.world_size}, state["step_count"])
                fabric.barrier()

            if not is_accumulating and state["step_count"] % save_step_interval == 0:
                checkpoint_path = out_dir / f"iter-{state['iter_num']:06d}-ckpt.pth"
                fabric.print(f"Saving checkpoint to {str(checkpoint_path)!r}")
                save_checkpoint_with_conditioner(fabric, checkpoint_path, state)
                fabric.barrier()  # ç¡®ä¿æ‰€æœ‰ rank åŒæ­¥

            if state["iter_num"] >= max_iters:
                break

        state["epoch"] = epoch + 1
        if state["iter_num"] >= max_iters:
            break

    # ========== è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆ checkpoint ==========
    final_checkpoint_path = out_dir / f"iter-{state['iter_num']:06d}-final-ckpt.pth"
    fabric.print(f"\nğŸ Training finished! Saving final checkpoint to {str(final_checkpoint_path)!r}")
    save_checkpoint_with_conditioner(fabric, final_checkpoint_path, state)
    fabric.barrier()

@torch.no_grad()
def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:
    fabric.print("Validating ...")
    model.eval()
    
    # è·å–åŸå§‹æ¨¡å‹å¹¶ä¿å­˜ conditioner çš„è®­ç»ƒçŠ¶æ€
    raw_model = model.module if hasattr(model, 'module') else model
    conditioner_was_training = False
    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
        conditioner_was_training = raw_model.conditioner.training
        raw_model.conditioner.eval()

    losses = torch.zeros(eval_iters, num_extrapol, device=fabric.device)
    for k, val_data in enumerate(val_dataloader):
        if k >= eval_iters:
            break

        # å¦‚æœæ˜¯ Sample_Dataset (dict with pc)ï¼Œéœ€è¦æå– pc
        # å¦‚æœæ˜¯ PackedDataset (tensor only)ï¼Œpc=Noneï¼ˆæ— æ¡ä»¶éªŒè¯ï¼‰
        pc = None
        if isinstance(val_data, dict):
            pc_list = val_data.get('pc', None) or val_data.get('pc_normal', None)
            if pc_list is not None and len(pc_list) > 0:
                pc = torch.stack(pc_list, dim=0).to(fabric.device)
            val_data = val_data.get('token_list_0', val_data)  # æå– token æ•°æ®

        for i, length in enumerate([4096, 8192, 12288, 16384]):   #[2048, 4096, 8192, 16384]
            input_ids = val_data[:, 0 : length].contiguous()
            targets = val_data[:, 1 : length + 1].contiguous()
            # ä¼ å…¥ pc å‚æ•°ï¼ˆå¯ä»¥æ˜¯ Noneï¼Œæ¨¡å‹ä¼šæ­£ç¡®å¤„ç†ï¼‰
            logits = model(input_ids, pc=pc).logits
            loss = chunked_cross_entropy(logits, targets, chunk_size=0)
            losses[k,i] = loss.item()

    out = losses.mean(0)
    model.train()
    
    # æ¢å¤ conditioner çš„è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æœä¹‹å‰æ˜¯è®­ç»ƒæ¨¡å¼ï¼‰
    if conditioner_was_training and hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
        raw_model.conditioner.train()
    
    return out

def create_dataloader(
    batch_size: int, block_size: int, data_dir: Path, fabric, shuffle: bool = True, seed: int = 12345, split="train"
) -> DataLoader:
    datasets = []
    data_config = train_data_config if split == "train" else val_data_config
    for prefix, _ in data_config:
        filenames = sorted(glob.glob(str(data_dir / f"{prefix}*")))
        random.seed(seed)
        random.shuffle(filenames)
        if split != "train":
            n_chunks = - (8 // -nodes) # ceil division
        else:
            n_chunks = 8

        dataset = PackedDataset(
            filenames,
            n_chunks=n_chunks,
            block_size=block_size,
            shuffle=shuffle,
            seed=seed+fabric.global_rank,
            num_processes=fabric.world_size,
            process_rank=fabric.global_rank,
        )
        datasets.append(dataset)

    if not datasets:
        raise RuntimeError(
            f"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset."
        )

    weights = [weight for _, weight in data_config]
    sum_weights = sum(weights)
    weights = [el / sum_weights for el in weights]

    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)

    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)

def create_dataloaders(
    batch_size: int,
    block_size: int,
    fabric,
    train_data_dir: Path = Path("data/redpajama_sample"),
    val_data_dir: Optional[Path] = None,
    seed: int = 12345,
) -> Tuple[DataLoader, DataLoader]:
    if use_sample_dataset:
        def collate_as_list(batch):
            out = {}
            for item in batch:
                for k, v in item.items():
                    out.setdefault(k, []).append(v)
            return out

        train_dataset = Sample_Dataset(point_num=81920, use_H5=False, use_uid=False)
        sampler = torch.utils.data.distributed.DistributedSampler(
            train_dataset,
            num_replicas=fabric.world_size,
            rank=fabric.global_rank,
            shuffle=True,
            drop_last=False,
        )
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            num_workers=4,
            pin_memory=True,
            collate_fn=collate_as_list,
            sampler=sampler,
        )
        val_dataloader = None
        return train_dataloader, val_dataloader
    else:
        effective_block_size = block_size + 1
        train_dataloader = create_dataloader(
            batch_size=batch_size,
            block_size=effective_block_size,
            fabric=fabric,
            data_dir=train_data_dir,
            shuffle=True,
            seed=seed,
            split="train"
        )
        val_dataloader = (
            create_dataloader(
                batch_size= - (batch_size // -2), # ceil division
                block_size=  16384 + 1,
                fabric=fabric,
                data_dir=val_data_dir,
                shuffle=False,
                seed=seed,
                split="validation"
            )
            if val_data_dir
            else None
        )
        return train_dataloader, val_dataloader

# learning rate decay scheduler (cosine with linear warmup)
def get_lr(it: int, warmup_iters: int, max_iters: int) -> float:
    if it < warmup_iters:
        return learning_rate * it / warmup_iters
    if it > max_iters:
        return min_lr
    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)

if __name__ == "__main__":
    # torch.backends.cuda.enable_flash_sdp(False)
    torch.set_float32_matmul_precision("high")
    from jsonargparse import CLI
    CLI(setup)
