# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# Copyright Lightning AI. Licensed under the Apache License 2.0,
# see LICENSE file at https://github.com/Lightning-AI/litgpt/blob/main/LICENSE

import glob
import math
import sys
import time
from pathlib import Path
from typing import List, Optional, Tuple, Union
import math
import lightning as L
import torch
from lightning.fabric.strategies import FSDPStrategy
from torch.utils.data import DataLoader
from functools import partial
# support running without installing as a package
wd = Path(__file__).parent.parent.resolve()
sys.path.append(str(wd))
# from apex.optimizers import FusedAdam #torch optimizer has a cuda backend, which is faster actually
from lit_gpt.model import GPT, Block, Config
from lit_gpt.packed_dataset import CombinedDataset, PackedDataset
# from sft.datasets.DatasetDEEMOS import Sample_Dataset
from lit_gpt.speed_monitor import SpeedMonitorFabric as Monitor
from lit_gpt.speed_monitor import estimate_flops
from lit_gpt.utils import chunked_cross_entropy, num_parameters
from pytorch_lightning.loggers import WandbLogger
from lit_gpt import FusedCrossEntropyLoss
from sft.datasets.serializaitonDEEMOS import deserialize
import random
import os
from datetime import datetime
import numpy as np
import trimesh
import warnings


# Dafei's import

sys.path.insert(0, str(Path(__file__).parent))
from src.utils.import_tools import load_dataset_from_config, load_model_from_config
from src.utils.latent_tools import init_model, to_latent
from omegaconf import OmegaConf

warnings.filterwarnings("ignore", message="When using.*NO_SHARD.*")

model_name = "Diff_LLaMA_551M" # change to "Samba_1.3B" for 1.3B model
train_config = "HY1024_tsz128x16k_100B_ScaleUp20k_unlockCondition" # chanage to "tsz512x4k_100B" for 1.3B model
name = train_config +"_" + model_name

out_dir = Path(os.getenv("LIGHTNING_ARTIFACTS_DIR", "out")) / name / f"Samba-DEEMOS-{datetime.now().strftime('%m-%d-%H')}"
PAD_TOKEN_ID = 4737
devices = torch.cuda.device_count() or 1
# æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰çš„ Sample_Datasetï¼ˆä¸åš paddingï¼Œå˜é•¿æ ·æœ¬ä»¥ list ç»„ç»‡ï¼‰
use_sample_dataset = True

# ========== ShapeVAE Conditioner è®­ç»ƒé…ç½® ==========
# æ˜¯å¦å†»ç»“ conditionerï¼ˆFalse è¡¨ç¤ºè§£é”è®­ç»ƒï¼‰
freeze_conditioner = False
# conditioner çš„å­¦ä¹ ç‡å€ç‡ï¼ˆç›¸å¯¹äºä¸»å­¦ä¹ ç‡ï¼‰
# é€šå¸¸ pretrained æ¨¡å‹å¾®è°ƒæ—¶ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
conditioner_lr_scale = 1.0

# ========== Checkpoint é…ç½® ==========
# FSDP state_dict ç±»å‹ï¼š
#   - "full": å®Œæ•´ state dictï¼ˆå…¼å®¹æ€§å¥½ï¼Œä½†ä¿å­˜/åŠ è½½æ…¢ï¼Œ20G ckpt å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
#   - "sharded": åˆ†ç‰‡ state dictï¼ˆå¿«ï¼Œä½† checkpoint åˆ†æ•£åœ¨å¤šä¸ªæ–‡ä»¶ï¼‰
# æ³¨æ„ï¼šåˆ‡æ¢ç±»å‹åï¼Œæ—§çš„ checkpoint å¯èƒ½æ— æ³•åŠ è½½
fsdp_state_dict_type = "full"  # é»˜è®¤ä½¿ç”¨ full ä¿æŒå…¼å®¹æ€§

# # Hyperparameters
# if "20B" in name:
#     # single node
#     nodes = 1 # max 8
#     max_tokens = int(1e11) // 5 # 20 billion
# elif "100B" in name:
#     # multi-node
#     nodes = 8 # max 8
#     max_tokens = int(1e11) # 100 billion

# if "512x4k" in name:
#     #4k
#     global_batch_size = 512 // nodes
#     micro_batch_size = 6
# elif "256x8k" in name:
#     #8k
#     global_batch_size = 256 // nodes
#     micro_batch_size = 4
# elif "128x16k" in name:
#     #16k
#     global_batch_size = 320 // nodes
#     micro_batch_size = 5
# elif "64x32k" in name:
#     #32k
#     global_batch_size = 64 // nodes
#     micro_batch_size = 1
# elif "1024x2k" in name:
#     #2k
#     global_batch_size = 1024 // nodes
#     micro_batch_size = 16

# overfit
max_tokens = 1e9
global_batch_size = 256
micro_batch_size = 32


learning_rate = 1e-4

total_evals = 400
warmup_tokens = int(max_tokens * 0.05)
log_step_interval = 10
# eval_iters = total_evals // micro_batch_size # 50 # 25
save_step_interval = 2500  # 500
eval_step_interval = 100000000000000

num_extrapol = 4

weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
decay_lr = True
min_lr = 1e-5

# è®­ç»ƒæ€»è½®æ•°ï¼ˆæŒ‰ epoch è®¡æ•°ï¼‰
num_epochs = 60


batch_size = global_batch_size // devices
gradient_accumulation_steps = batch_size // micro_batch_size
assert gradient_accumulation_steps > 0

# log_iter_interval = log_step_interval * gradient_accumulation_steps
log_iter_interval = log_step_interval

# Treat all dataset equally by their size. If you want to use a different weight for a dataset, add it to the list with the weight.
# train_data_config = [
#     ("train_slim", 1.0),
# ]

# val_data_config = [
#     ("validation", 1.0),
# ]

hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str, bool)) and not k.startswith("_")}

wandb_logger = WandbLogger(project="Pretrain-LLM-Hourglass-551M-DEEMOS", entity="ruixu-hku")

# ---------------------------
# Chamfer Distance è®¡ç®—å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
# ---------------------------
def sample_points_from_mesh(vertices, faces, num_samples=1024):
    """
    ä» mesh è¡¨é¢éšæœºé‡‡æ ·ç‚¹
    
    Args:
        vertices: (N, 3) numpy array
        faces: (M, 3) numpy array
        num_samples: é‡‡æ ·ç‚¹æ•°
    
    Returns:
        sampled_points: (num_samples, 3) numpy array
    """
    try:
        # åˆ›å»º trimesh å¯¹è±¡
        mesh = trimesh.Trimesh(vertices=vertices, faces=faces, process=False)
        
        # ä»è¡¨é¢é‡‡æ ·ç‚¹
        sampled_points, _ = trimesh.sample.sample_surface(mesh, num_samples)
        
        return sampled_points
    except Exception as e:
        # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨é¡¶ç‚¹å¹¶éšæœºé‡‡æ ·
        if len(vertices) >= num_samples:
            indices = np.random.choice(len(vertices), num_samples, replace=False)
            return vertices[indices]
        else:
            # é¡¶ç‚¹ä¸å¤Ÿï¼Œé‡å¤é‡‡æ ·
            indices = np.random.choice(len(vertices), num_samples, replace=True)
            return vertices[indices]


def compute_chamfer_distance_fast(pred_points, gt_points):
    """
    å¿«é€Ÿè®¡ç®—ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„ Chamfer Distance
    
    Args:
        pred_points: (N, 3) numpy array æˆ– torch tensor
        gt_points: (M, 3) numpy array æˆ– torch tensor
    
    Returns:
        chamfer_dist: åŒå‘ Chamfer Distance
    """
    try:
        # è½¬æ¢ä¸º torch tensorï¼ˆåœ¨ GPU ä¸Šï¼‰
        if not torch.is_tensor(pred_points):
            pred_points = torch.from_numpy(pred_points).float()
        if not torch.is_tensor(gt_points):
            gt_points = torch.from_numpy(gt_points).float()
        
        # ç¡®ä¿åœ¨ GPU ä¸Š
        if not pred_points.is_cuda:
            pred_points = pred_points.cuda()
        if not gt_points.is_cuda:
            gt_points = gt_points.cuda()
        
        # è®¡ç®—åŒå‘ Chamfer Distance
        # pred -> gt
        dist_matrix = torch.cdist(pred_points.unsqueeze(0), gt_points.unsqueeze(0), p=2).squeeze(0)  # (N, M)
        min_dist_pred_to_gt = dist_matrix.min(dim=1)[0]  # (N,)
        
        # gt -> pred
        min_dist_gt_to_pred = dist_matrix.min(dim=0)[0]  # (M,)
        
        # Chamfer Distance (åŒå‘å¹³å‡)
        chamfer_dist = (min_dist_pred_to_gt.mean() + min_dist_gt_to_pred.mean()).item()
        
        return chamfer_dist
    
    except Exception as e:
        return float('inf')


def validate_and_filter_faces(vertices, faces):
    """éªŒè¯å¹¶è¿‡æ»¤ facesï¼Œç§»é™¤åŒ…å«è¶…å‡º vertices èŒƒå›´ç´¢å¼•çš„ face"""
    if len(vertices) == 0 or len(faces) == 0:
        return np.array([])
    
    num_vertices = len(vertices)
    max_valid_idx = num_vertices - 1
    valid_mask = np.all((faces >= 0) & (faces <= max_valid_idx), axis=1)
    
    filtered_faces = faces[valid_mask]
    return filtered_faces


def tokens_to_mesh_with_sampling(tokens, pad_id=4737, num_samples=1024):
    """
    å°† token åºåˆ—è§£ç ä¸º meshï¼Œå¹¶ä»è¡¨é¢é‡‡æ ·ç‚¹
    
    Args:
        tokens: token åºåˆ— (numpy array æˆ– torch tensor)
        pad_id: padding token id
        num_samples: é‡‡æ ·ç‚¹æ•°
    
    Returns:
        sampled_points: (num_samples, 3) numpy arrayï¼Œå¦‚æœè§£ç å¤±è´¥è¿”å› None
    """
    try:
        # è½¬ä¸º numpy
        if torch.is_tensor(tokens):
            tokens = tokens.detach().cpu().numpy()
        
        # ç§»é™¤ padding
        tokens = tokens[tokens != pad_id]
        
        if len(tokens) == 0:
            return None
        
        # è§£ç ä¸º mesh
        vertices, faces = deserialize(tokens)
        
        if len(vertices) == 0:
            return None
        
        # è¿‡æ»¤æ— æ•ˆçš„ faces
        faces = faces.reshape(-1, 3)
        faces = validate_and_filter_faces(vertices, faces)
        
        
        if len(faces) == 0:
            # æ²¡æœ‰æœ‰æ•ˆçš„ faceï¼Œç›´æ¥ä½¿ç”¨é¡¶ç‚¹
            if len(vertices) >= num_samples:
                indices = np.random.choice(len(vertices), num_samples, replace=False)
                return vertices[indices]
            else:
                indices = np.random.choice(len(vertices), num_samples, replace=True)
                return vertices[indices]
        
        # ä»è¡¨é¢é‡‡æ ·ç‚¹
        sampled_points = sample_points_from_mesh(vertices, faces, num_samples)
        
        return sampled_points
    
    except Exception as e:
        return None

# ---------------------------
# æ–°å¢ï¼šä¸¥æ ¼æ¬è¿æ¨¡å—ï¼ˆå«æœªæ³¨å†Œ tensorï¼‰
# ---------------------------
def move_module_strict(module: torch.nn.Module, device: torch.device, dtype: torch.dtype | None = None):
    """æŠŠ module çš„å‚æ•°ã€buffers ä»¥åŠæœªæ³¨å†Œçš„è£¸ tensor å±æ€§éƒ½æ¬åˆ° device/dtypeã€‚"""
    module.to(device=device, dtype=dtype)
    for sub in module.modules():
        param_names = set(n for n, _ in sub.named_parameters(recurse=False))
        buffer_names = set(n for n, _ in sub.named_buffers(recurse=False))
        for name, value in vars(sub).items():
            if name in param_names or name in buffer_names:
                continue
            if isinstance(value, torch.Tensor):
                setattr(sub, name, value.to(device=device, dtype=(dtype or value.dtype)))


# ---------------------------
# Conditioner Checkpoint è¾…åŠ©å‡½æ•°ï¼ˆç»Ÿä¸€ä¿å­˜/åŠ è½½ï¼‰
# ---------------------------
def save_checkpoint_with_conditioner(fabric, checkpoint_path: Path, state: dict) -> None:
    """
    ä¿å­˜ checkpointï¼ŒåŒ…å«ä¸»æ¨¡å‹å’Œ conditionerã€‚
    ç”±äº conditioner åœ¨ FSDP ignored_modules ä¸­ï¼Œéœ€è¦æ‰‹åŠ¨ä¿å­˜ã€‚
    """
    model = state["model"]
    raw_model = model.module if hasattr(model, 'module') else model
    
    # åœ¨ä¿å­˜å‰ï¼Œæ‰‹åŠ¨å°† conditioner çš„ state_dict åŠ å…¥åˆ° state ä¸­
    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
        # åªåœ¨ rank 0 å‡†å¤‡ conditioner state
        if fabric.global_rank == 0:
            state['conditioner_state_dict'] = raw_model.conditioner.state_dict()
            state['freeze_conditioner'] = freeze_conditioner
    
    # ä¿å­˜å®Œæ•´çš„ stateï¼ˆåŒ…å« conditionerï¼‰
    fabric.save(checkpoint_path, state)
    
    # æ¸…ç†ä¸´æ—¶æ·»åŠ çš„ key
    if 'conditioner_state_dict' in state:
        del state['conditioner_state_dict']
    if 'freeze_conditioner' in state:
        del state['freeze_conditioner']
    
    if fabric.global_rank == 0:
        fabric.print(f"ğŸ’¾ Checkpoint saved to {str(checkpoint_path)!r}")
        if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
            fabric.print(f"   âœ… Conditioner included in checkpoint")


def load_checkpoint_with_conditioner(fabric, checkpoint_path: Path, state: dict, model) -> None:
    """
    åŠ è½½ checkpointï¼ŒåŒ…å«ä¸»æ¨¡å‹å’Œ conditionerã€‚
    """
    # åŠ è½½ checkpoint
    fabric.load(checkpoint_path, state)
    
    raw_model = model.module if hasattr(model, 'module') else model
    
    # å°è¯•åŠ è½½ conditionerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
        # æ£€æŸ¥ checkpoint ä¸­æ˜¯å¦æœ‰ conditioner
        if 'conditioner_state_dict' in state:
            if fabric.global_rank == 0:
                fabric.print(f"ğŸ“‚ Loading conditioner from checkpoint...")
            
            conditioner_state = state['conditioner_state_dict']
            saved_freeze = state.get('freeze_conditioner', True)
            
            # å¹¿æ’­ç»™æ‰€æœ‰ ranksï¼ˆå¦‚æœæ˜¯å¤šGPUï¼‰
            if fabric.world_size > 1:
                object_list = [conditioner_state]
                torch.distributed.broadcast_object_list(object_list, src=0)
                conditioner_state = object_list[0]
            
            # åŠ è½½ state_dict
            raw_model.conditioner.load_state_dict(conditioner_state)
            
            # ç¡®ä¿ç²¾åº¦ä¸€è‡´ï¼ˆè½¬ä¸º fp32ï¼‰
            if next(raw_model.conditioner.parameters()).device != fabric.device:
                move_module_strict(raw_model.conditioner, fabric.device)
            
            for param in raw_model.conditioner.parameters():
                if param.dtype != torch.float32:
                    param.data = param.data.to(torch.float32)
            
            for buffer in raw_model.conditioner.buffers():
                if buffer.dtype not in [torch.long, torch.int, torch.bool]:
                    if buffer.dtype != torch.float32:
                        buffer.data = buffer.data.to(torch.float32)
            
            # æ¸…ç†ä¸´æ—¶ key
            del state['conditioner_state_dict']
            if 'freeze_conditioner' in state:
                del state['freeze_conditioner']
            
            if fabric.global_rank == 0:
                fabric.print(f"âœ… Conditioner loaded successfully!")
                fabric.print(f"   â””â”€â”€ Was saved with freeze_conditioner={saved_freeze}, current={freeze_conditioner}")
        else:
            if fabric.global_rank == 0:
                fabric.print(f"âš ï¸  No conditioner found in checkpoint")
                fabric.print(f"   ShapeVAE will use default initialization (train from scratch or resume)")

def setup(
    config_path: Optional[str] = None,
    train_data_dir: Path = Path("data/redpajama_sample"),
    val_data_dir: Optional[Path] = None,
    resume: Union[bool, Path] = False,
    warm_start_ckpt: Optional[Path] = None, 
) -> None:
    # ========== åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰ ==========
    config_dict = None
    if config_path is not None and Path(config_path).exists():
        config_dict = OmegaConf.load(config_path)
        print(f"ğŸ“‚ Loaded config from: {config_path}")
    elif config_path is not None:
        print(f"âš ï¸  Config file not found: {config_path}, using default settings")
    
    # ========== åŠ è½½æ¨¡å‹ ==========
    if config_dict is not None and "model" in config_dict:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹
        print("ğŸ“¦ Loading model from config...")
        model = load_model_from_config(config_dict, device=None, strict=False)
        
        # è·å–æ¨¡å‹é…ç½®ï¼ˆå¦‚æœæ˜¯ GPT æ¨¡å‹ï¼‰
        if hasattr(model, 'config'):
            config = model.config
        else:
            # å¦‚æœæ¨¡å‹æ²¡æœ‰ config å±æ€§ï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶åˆ›å»º
            model_name_from_config = config_dict.get("model", {}).get("params", {}).get("model_name", model_name)
            config = Config.from_name(model_name_from_config)
            config.padded_vocab_size = (2*4**3) + (8**3) + (16**3) + 1 + 1
            config.block_size = 270000
        
        # ä»é…ç½®æ–‡ä»¶è¯»å– freeze_conditionerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "freeze_conditioner" in config_dict:
            freeze_conditioner = config_dict.get("freeze_conditioner", False)
            print(f"ğŸ“‹ Using freeze_conditioner from config: {freeze_conditioner}")
    else:
        # å‘åå…¼å®¹ï¼šä½¿ç”¨ç¡¬ç¼–ç æ–¹å¼åˆ›å»ºæ¨¡å‹
        print("ğŸ“¦ Loading model using legacy method...")
        config = Config.from_name(model_name)
        # config.padded_vocab_size = (2*4**3) + (8**3) + (16**3) + 1 + 1  # 4736 + 2
        config.padded_vocab_size = 1026  # 4736 + 2
        config.block_size = 1000
        # config.block_size = 270000

        # æ ¹æ® freeze_conditioner é…ç½®å†³å®šæ˜¯å¦å†»ç»“ conditioner
        model = GPT(config, freeze_conditioner=False, build_conditioner=False)
        model.apply(partial(model._init_weights, n_layer=config.n_layer))

    # å¯é€‰ï¼šä»æ—§ckptè¿›è¡Œwarm-startï¼Œä»…åŠ è½½åŒ¹é…æƒé‡ï¼ˆå¿½ç•¥å¤šå‡ºæ¥çš„æ–°å±‚ï¼‰
    if warm_start_ckpt is not None:
        try:
            ckpt = torch.load(warm_start_ckpt, map_location="cpu")
            model_state = ckpt.get("model", ckpt)
            missing, unexpected = model.load_state_dict(model_state, strict=False)
            print(f"Warm-start loaded with strict=False. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
        except Exception as e:
            print(f"Warm-start failed: {e}")

    # 2) æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å†»ç»“ conditioner
    if hasattr(model, "conditioner") and isinstance(model.conditioner, torch.nn.Module):
        if freeze_conditioner:
            # å†»ç»“ conditioner
            for p in model.conditioner.parameters():
                p.requires_grad = False
            model.conditioner.eval()
            print("ğŸ”’ Conditioner is FROZEN (not trainable)")
        else:
            # è§£é” conditionerï¼Œä¿æŒ train æ¨¡å¼
            for p in model.conditioner.parameters():
                p.requires_grad = True
            # é€’å½’è®¾ç½®æ‰€æœ‰å­æ¨¡å—ä¸º train æ¨¡å¼ï¼ˆåŒ…æ‹¬ BatchNorm/LayerNormï¼‰
            def set_train_recursive(module):
                module.train()
                for child in module.children():
                    set_train_recursive(child)
            set_train_recursive(model.conditioner)
            print("ğŸ”“ Conditioner is UNFROZEN (trainable)")
            print(f"   Total conditioner params: {sum(p.numel() for p in model.conditioner.parameters()):,}")

    # 3) å‡†å¤‡ ignored_modules
    # æ³¨æ„ï¼šå³ä½¿ conditioner å‚ä¸è®­ç»ƒï¼Œæˆ‘ä»¬ä»ç„¶å°†å…¶æ”¾å…¥ ignored_modules
    # è¿™æ ·å¯ä»¥é¿å… FSDP åŒ…è£… ShapeVAE çš„å¤æ‚ç»“æ„ï¼ŒåŒæ—¶æ¢¯åº¦ä»ç„¶å¯ä»¥æ­£å¸¸æµåŠ¨
    # conditioner çš„å‚æ•°ä¼šç”± DataParallel-like æ–¹å¼å¤„ç†ï¼ˆæ¯ä¸ª rank å®Œæ•´å‰¯æœ¬ï¼‰
    ignored = [m for m in [getattr(model, "conditioner", None)] if isinstance(m, torch.nn.Module)]

    # 4) åˆ›å»º FSDPStrategyï¼ˆåˆå§‹åŒ–æ—¶å°±ä¼ å…¥ ignored_modulesï¼‰
    strategy = FSDPStrategy(
        auto_wrap_policy={Block},
        state_dict_type=fsdp_state_dict_type,  # å¯é…ç½®ï¼šfullï¼ˆæ…¢ä½†å…¼å®¹ï¼‰æˆ– shardedï¼ˆå¿«ï¼‰
        ignored_modules=ignored,
        use_orig_params=True,
        # cpu_offload=True,
    )

    # 5) åˆ›å»º Fabric å¹¶ launch
    fabric = L.Fabric(
        devices=devices,
        strategy=strategy,
        precision="bf16-mixed",
        loggers=[wandb_logger],
    )
    fabric.launch()

    # 6) ========== ç²¾åº¦ç®¡ç†ç­–ç•¥ï¼ˆç»Ÿä¸€ä¸º fp32 å‚æ•° + bf16 è®¡ç®—ï¼‰==========
    # ç›®æ ‡ï¼šè®© conditioner ä¸ä¸»ç½‘ç»œä¿æŒç›¸åŒçš„ mixed precision ç­–ç•¥
    # - å‚æ•°å­˜å‚¨ï¼šfp32ï¼ˆé«˜ç²¾åº¦ï¼Œé¿å…ç´¯ç§¯è¯¯å·®ï¼‰
    # - å‰å‘è®¡ç®—ï¼šbf16ï¼ˆè‡ªåŠ¨è½¬æ¢ï¼Œåˆ©ç”¨ Tensor Coreï¼‰
    # - æ¢¯åº¦ç´¯ç§¯ï¼šfp32ï¼ˆæ•°å€¼ç¨³å®šï¼‰
    # - ä¼˜åŒ–å™¨çŠ¶æ€ï¼šfp32ï¼ˆAdam åŠ¨é‡/æ–¹å·®ï¼‰
    if hasattr(model, "conditioner") and isinstance(model.conditioner, torch.nn.Module):
        # ç­–ç•¥1: ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        move_module_strict(model.conditioner, fabric.device)
        
        # ç­–ç•¥2: ç»Ÿä¸€è½¬æ¢ä¸º fp32ï¼ˆä¸ FSDP ä¸»ç½‘ç»œä¸€è‡´ï¼‰
        # âš ï¸ æ³¨æ„ï¼šä¸è¦è½¬ä¸º bf16ï¼é‚£ä¼šå¯¼è‡´æ¢¯åº¦ä¹Ÿæ˜¯ bf16ï¼Œç²¾åº¦ä¸è¶³
        for name, param in model.conditioner.named_parameters():
            if param.dtype != torch.float32:
                param.data = param.data.to(torch.float32)
        
        # ç­–ç•¥3: è½¬æ¢æ‰€æœ‰ buffers ä¸º fp32
        for name, buffer in model.conditioner.named_buffers():
            if buffer.dtype not in [torch.long, torch.int, torch.bool]:  # ä¿ç•™æ•´æ•°ç±»å‹
                if buffer.dtype != torch.float32:
                    buffer.data = buffer.data.to(torch.float32)
        
        print(f"âœ… Conditioner precision unified to fp32 (same as main network)")
        print(f"   Device: {fabric.device}")
        print(f"   Params dtype: {next(model.conditioner.parameters()).dtype}")
        print(f"   Training: bf16-mixed (fp32 params â†’ bf16 compute â†’ fp32 grads)")
        print(f"   Memory overhead: ~{sum(p.numel() for p in model.conditioner.parameters()) * 2 / 1024**2:.1f} MB (vs bf16)")

    # 7) ç»Ÿä¸€é-conditioner å‚æ•°ä¸º fp32ï¼Œé˜²æ­¢ FSDP æ‰å¹³åŒ–æ—¶æŠ¥ "mixed dtypes"
    def cast_non_conditioner_fp32(m: torch.nn.Module):
        cond = getattr(m, "conditioner", None)
        for sub in m.modules():
            if sub is cond:
                continue
            for p in sub.parameters(recurse=False):
                if p.dtype != torch.float32:
                    p.data = p.data.to(torch.float32)
    # cast_non_conditioner_fp32(model)

    # 8) è¿›å…¥ä¸»æµç¨‹
    main(fabric, model, config_dict, train_data_dir, val_data_dir, resume)

def main(fabric, model, config_dict, train_data_dir, val_data_dir, resume, **overides):
    monitor = Monitor(fabric, window_size=1, time_unit="seconds", log_iter_interval=log_iter_interval)

    if fabric.global_rank == 0:
        out_dir.mkdir(parents=True, exist_ok=True)

    fabric.seed_everything(42)
    # è¿™é‡Œä¸å† from_name/é‡æ–°å»ºæ¨¡äº†ï¼Œç›´æ¥ç”¨ä¼ è¿›æ¥çš„ model
    config = model.config

    train_dataloader, val_dataloader = create_dataloaders(
        batch_size=micro_batch_size,
        fabric=fabric,
        config_dict=config_dict,
        seed=42,
    )

    if val_dataloader is None:
        train_dataloader = fabric.setup_dataloaders(train_dataloader)
    else:
        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)

    

    fabric.print(f"Loading model with {config.__dict__}")
    fabric.print(f"Total parameters {num_parameters(model):,}")
    fabric.print(model)

    # ç»Ÿä¸€ç”± Fabric/FSDP æ¬åˆ°å„è‡ª rank çš„è®¾å¤‡
    model = fabric.setup(model)

    # ========== æ„å»ºä¼˜åŒ–å™¨ï¼ˆåŒºåˆ† conditioner å’Œä¸»å¹²çš„å­¦ä¹ ç‡ï¼‰ ==========
    if not freeze_conditioner and hasattr(model, "conditioner") and model.conditioner is not None:
        # è·å–åŸå§‹æ¨¡å‹ï¼ˆå¤„ç† FSDP åŒ…è£…ï¼‰
        raw_model = model.module if hasattr(model, 'module') else model
        
        # åˆ†ç¦» conditioner å‚æ•°å’Œå…¶ä»–å‚æ•°
        conditioner_params = []
        other_params = []
        conditioner_param_ids = set(id(p) for p in raw_model.conditioner.parameters())
        
        for name, param in raw_model.named_parameters():
            if param.requires_grad:
                if id(param) in conditioner_param_ids:
                    conditioner_params.append(param)
                else:
                    other_params.append(param)
        
        # ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡çš„å‚æ•°ç»„
        param_groups = [
            {"params": other_params, "lr": learning_rate},
            {"params": conditioner_params, "lr": learning_rate * conditioner_lr_scale, "name": "conditioner"},
        ]
        
        fabric.print(f"ğŸ”§ Optimizer setup with separate learning rates:")
        fabric.print(f"   - Main model params: {len(other_params)}, lr={learning_rate}")
        fabric.print(f"   - Conditioner params: {len(conditioner_params)}, lr={learning_rate * conditioner_lr_scale}")
        
        optimizer = torch.optim.AdamW(
            param_groups, weight_decay=weight_decay, betas=(beta1, beta2), fused=True
        )
    else:
        # åŸæœ‰é€»è¾‘ï¼šæ‰€æœ‰å‚æ•°ä½¿ç”¨ç›¸åŒå­¦ä¹ ç‡
        optimizer = torch.optim.AdamW(
            model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), fused=True
        )
    
    optimizer = fabric.setup_optimizers(optimizer)

    state = {"model": model, "optimizer": optimizer, "hparams": hparams, "iter_num": 0, "step_count": 0, "epoch": 0}

    if resume is True:
        resume = sorted(out_dir.glob("*.pth"))[-1]
    if resume:
        fabric.print(f"\n{'='*60}")
        fabric.print(f"ğŸ“¥ Resuming training from {resume}")
        fabric.print(f"   FSDP state_dict_type: {fsdp_state_dict_type}")
        fabric.print(f"   Current freeze_conditioner: {freeze_conditioner}")
        fabric.print(f"{'='*60}")
        
        # è®°å½•åŠ è½½æ—¶é—´
        t0 = time.perf_counter()
        resume_path = Path(resume) if not isinstance(resume, Path) else resume
        
        # ========== æ™ºèƒ½åŠ è½½ï¼šå¤„ç† optimizer å‚æ•°ç»„ä¸åŒ¹é… ==========
        skip_optimizer = False
        if fabric.global_rank == 0:
            try:
                ckpt_preview = torch.load(resume_path, map_location='cpu', weights_only=False)
                if 'optimizer' in ckpt_preview:
                    saved_param_groups = len(ckpt_preview['optimizer'].get('param_groups', []))
                    current_param_groups = len(optimizer.param_groups)
                    if saved_param_groups != current_param_groups:
                        fabric.print(f"âš ï¸  Optimizer param groups mismatch: saved={saved_param_groups}, current={current_param_groups}")
                        fabric.print(f"   This is normal when switching freeze_conditioner setting.")
                        fabric.print(f"   Will skip optimizer state and re-initialize.")
                        skip_optimizer = True
                del ckpt_preview
            except Exception as e:
                fabric.print(f"âš ï¸  Could not preview checkpoint: {e}")
        
        # å¹¿æ’­ skip_optimizer ç»™æ‰€æœ‰ rank
        if fabric.world_size > 1:
            skip_tensor = torch.tensor([skip_optimizer], dtype=torch.int32, device=fabric.device)
            torch.distributed.broadcast(skip_tensor, src=0)
            skip_optimizer = bool(skip_tensor.item())
        
        if skip_optimizer:
            # åªåŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸åŠ è½½ optimizer
            state_model_only = {"model": model, "iter_num": 0, "step_count": 0, "epoch": 0}
            load_checkpoint_with_conditioner(fabric, resume_path, state_model_only, model)
            
            # æ¢å¤è®­ç»ƒçŠ¶æ€
            state["iter_num"] = state_model_only.get("iter_num", 0)
            state["step_count"] = state_model_only.get("step_count", 0)
            state["epoch"] = state_model_only.get("epoch", 0)
            
            fabric.print(f"âœ… Model weights loaded, optimizer re-initialized")
            fabric.print(f"   Resumed from iter={state['iter_num']}, step={state['step_count']}, epoch={state['epoch']}")
        else:
            # æ­£å¸¸åŠ è½½ï¼ˆåŒ…å« optimizerï¼‰
            load_checkpoint_with_conditioner(fabric, resume_path, state, model)
            fabric.print(f"âœ… Full checkpoint loaded (model + optimizer + conditioner)")
        
        t1 = time.perf_counter()
        fabric.print(f"â±ï¸  Total resume time: {t1 - t0:.2f}s")
        fabric.print(f"{'='*60}\n")
        
        fabric.barrier()  # ç¡®ä¿æ‰€æœ‰ rank åŒæ­¥

    train_time = time.perf_counter()
    train(fabric, state, train_dataloader, val_dataloader, monitor, resume)
    fabric.print(f"Training time: {(time.perf_counter()-train_time):.2f}s")
    if fabric.device.type == "cuda":
        fabric.print(f"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB")

def train(fabric, state, train_dataloader, val_dataloader, monitor, resume):
    model = state["model"]
    optimizer = state["optimizer"]

    # if val_dataloader is not None:
    #     validate(fabric, model, val_dataloader)  # sanity check

    # ------- ä»ç„¶åœ¨ meta ä¸Šä¼° FLOPsï¼Œä½†å…³é—­ conditioner -------
    with torch.device("meta"):
        meta_model = GPT(model.config, build_conditioner=False)
        estimated_flops = estimate_flops(meta_model) * micro_batch_size
        fabric.print(f"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}")
        x = torch.randint(0, 1, (micro_batch_size, model.config.block_size))
        del meta_model, x
    # ------------------------------------------------------

    total_lengths = 0
    total_t0 = time.perf_counter()

    if fabric.device.type == "xla":
        import torch_xla.core.xla_model as xm
        xm.mark_step()

    # ä½¿ç”¨ epoch è®¡æ•°çš„è°ƒåº¦å‚æ•°
    steps_per_epoch = len(train_dataloader)
    total_iters = steps_per_epoch * max(1, num_epochs)
    max_iters = total_iters
    warmup_iters = max(1, int(0.01 * total_iters)) if decay_lr else 0
    initial_iter = state["iter_num"]

    loss_func = FusedCrossEntropyLoss()
    
    # è®­ç»ƒå‰è¯Šæ–­ï¼šæ£€æŸ¥ condition æµç¨‹
    if fabric.global_rank == 0 and state["iter_num"] == 0:
        fabric.print("\n" + "="*60)
        fabric.print("ğŸ” Condition Pipeline Diagnostic Check")
        fabric.print("="*60)
        raw_model = model.module if hasattr(model, 'module') else model
        if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
            is_frozen = not any(p.requires_grad for p in raw_model.conditioner.parameters())
            fabric.print("âœ… Conditioner found (ShapeVAE from config.yaml)")
            fabric.print(f"   - Status: {'ğŸ”’ FROZEN' if is_frozen else 'ğŸ”“ TRAINABLE'}")
            fabric.print(f"   - Mode: {'eval' if not raw_model.conditioner.training else 'train'}")
            fabric.print(f"   - Dtype: {next(raw_model.conditioner.parameters()).dtype}")
            fabric.print(f"   - Device: {next(raw_model.conditioner.parameters()).device}")
            
            if not is_frozen:
                cond_params = sum(p.numel() for p in raw_model.conditioner.parameters() if p.requires_grad)
                fabric.print(f"   - Trainable params: {cond_params:,}")
                fabric.print(f"   - Learning rate scale: {conditioner_lr_scale}x")
            
            # éªŒè¯ ShapeVAE çš„ç¼–è§£ç æµç¨‹
            fabric.print(f"\nğŸ“‹ ShapeVAE Architecture:")
            fabric.print(f"   - Latent shape: {raw_model.conditioner.latent_shape}")
            fabric.print(f"   - Encoder: PointCrossAttentionEncoder")
            fabric.print(f"   - Decoder: post_kl + Transformer")
            fabric.print(f"   - Expected workflow: pc â†’ encode() â†’ latent_codes â†’ decode() â†’ features")
            
            # æ£€æŸ¥å„ç»„ä»¶çš„ dtype
            fabric.print(f"\nğŸ”§ Component Dtypes:")
            if hasattr(raw_model.conditioner, 'encoder'):
                fabric.print(f"   - Encoder: {next(raw_model.conditioner.encoder.parameters()).dtype}")
            if hasattr(raw_model.conditioner, 'pre_kl'):
                fabric.print(f"   - Pre-KL: {raw_model.conditioner.pre_kl.weight.dtype}")
            if hasattr(raw_model.conditioner, 'post_kl'):
                fabric.print(f"   - Post-KL: {raw_model.conditioner.post_kl.weight.dtype}")
            if hasattr(raw_model.conditioner, 'transformer'):
                fabric.print(f"   - Transformer: {next(raw_model.conditioner.transformer.parameters()).dtype}")
            
            if hasattr(raw_model, 'linear'):
                fabric.print(f"\nâœ… Linear projection layer found")
                fabric.print(f"   - input_dim: {raw_model.linear.in_features}")
                fabric.print(f"   - output_dim: {raw_model.linear.out_features}")
                fabric.print(f"   - dtype: {raw_model.linear.weight.dtype}")
                fabric.print(f"   - trainable: {raw_model.linear.weight.requires_grad}")
            
            # ç»Ÿè®¡ cross-attention å±‚æ•°é‡
            cross_attn_count = sum(1 for name, _ in raw_model.named_modules() if 'cross_attn' in name)
            fabric.print(f"\nâœ… Found {cross_attn_count} CrossAttention layers")
            
            # æ ¹æ®é™é‡‡æ ·é…ç½®è®¡ç®—æœŸæœ›çš„ token æ•°
            num_latents = raw_model.conditioner.latent_shape[0]
            if hasattr(raw_model, 'condition_downsample_factor'):
                expected_tokens = num_latents // raw_model.condition_downsample_factor
                fabric.print(f"   Expected condition tokens: {expected_tokens} ({num_latents} â†’ downsample by {raw_model.condition_downsample_factor}x)")
                if hasattr(raw_model, 'condition_downsample') and raw_model.condition_downsample is not None:
                    fabric.print(f"   Downsample method: learnable (MLP)")
                else:
                    fabric.print(f"   Downsample method: average pooling")
            else:
                fabric.print(f"   Expected condition tokens: {num_latents} (no downsampling)")
        else:
            fabric.print("âš ï¸  WARNING: No conditioner found!")
        fabric.print("="*60 + "\n")
    
    # ä»¥ epoch ä¸ºå•ä½è®­ç»ƒ
    for epoch in range(state.get("epoch", 0), max(1, num_epochs)):
        try:
            sampler = getattr(train_dataloader, 'sampler', None)
            if hasattr(sampler, 'set_epoch'):
                sampler.set_epoch(epoch)
        except Exception:
            pass
        idddx = 0
        for train_data in train_dataloader:
            idddx += 1
            if state["iter_num"] >= max_iters:
                break

            # determine and set the learning rate for this iteration
            lr = get_lr(state["iter_num"], warmup_iters, max_iters) if decay_lr else learning_rate
            for param_group in optimizer.param_groups:
                # å¦‚æœæ˜¯ conditioner å‚æ•°ç»„ï¼Œä½¿ç”¨ç¼©æ”¾çš„å­¦ä¹ ç‡
                if param_group.get("name") == "conditioner":
                    param_group["lr"] = lr * conditioner_lr_scale
                else:
                    param_group["lr"] = lr

            iter_t0 = time.perf_counter()

            # å¤„ç†ç‚¹äº‘ pcï¼ˆå¯é€‰ï¼Œä¸å‚ä¸ GPT æŸå¤±ï¼Œä½†ç§»åŠ¨åˆ°å½“å‰è®¾å¤‡ï¼‰
            pc_list = None
            if isinstance(train_data, dict):
                pc_list = train_data.get('pc', None)
                if pc_list is None:
                    pc_list = train_data.get('pc_normal', None)
            
            # æ£€æŸ¥ pc_list æ˜¯å¦ä¸º None æˆ–ç©ºåˆ—è¡¨ï¼Œé¿å… torch.stack å´©æºƒ
            if pc_list is None or len(pc_list) == 0:
                fabric.print(f"Warning: pc_list is None or empty at iter {state['iter_num']}. Skipping this batch.")
                state["iter_num"] += 1
                continue
            
            pc = torch.stack(pc_list, dim=0).to(fabric.device)  # ç¡®ä¿ä¸æ¨¡å‹åŒè®¾å¤‡
            
            # æ£€æŸ¥ pc çš„å½¢çŠ¶
            if pc.dim() != 3 or pc.shape[1] != 81920 or pc.shape[2] != 7:
                fabric.print(f"Warning: pc has unexpected shape {pc.shape} at iter {state['iter_num']}. Skipping this batch.")
                state["iter_num"] += 1
                continue
            # å¤„ç† tokensï¼špadding/æˆªæ–­åˆ°å›ºå®šé•¿åº¦
            token_lists = train_data['token_list_0'] if isinstance(train_data, dict) and 'token_list_0' in train_data else None
            maxL = 0
            minL = 9999999
            lengths = []  # è®°å½•æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦ï¼ˆæˆªæ–­åï¼‰
            if token_lists is not None and len(token_lists) > 0:
                pad_id = 4737  # ä½¿ç”¨ EOS_TOKEN_ID ä½œä¸º padding
                max_len = 9001  # å›ºå®šé•¿åº¦ä¸Šé™
                padded_token_tensors = []
                
                for t in token_lists:
                    t_tensor = t if torch.is_tensor(t) else torch.tensor(t, dtype=torch.long)
                    orig_len = t_tensor.numel()
                    
                    # è®°å½•åŸå§‹ç»Ÿè®¡ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
                    maxL = max(maxL, orig_len)
                    minL = min(minL, orig_len)
                    
                    # æˆªæ–­æˆ– padding åˆ° max_len
                    if orig_len >= max_len:
                        # éœ€è¦æˆªæ–­
                        padded_t = t_tensor[:max_len]
                        L_eff = max_len  # æœ‰æ•ˆé•¿åº¦ = max_len
                    else:
                        # éœ€è¦ padding
                        pad_len = max_len - orig_len
                        pad = torch.full((pad_len,), pad_id, dtype=torch.long, device=t_tensor.device)
                        padded_t = torch.cat([t_tensor, pad], dim=0)
                        L_eff = orig_len  # æœ‰æ•ˆé•¿åº¦ = åŸå§‹é•¿åº¦
                    
                    padded_token_tensors.append(padded_t)
                    lengths.append(L_eff)  # è®°å½•æˆªæ–­åçš„æœ‰æ•ˆé•¿åº¦
                
                merged_token_tensor = torch.stack(padded_token_tensors, dim=0).to(fabric.device)
                lengths = torch.tensor(lengths, device=fabric.device, dtype=torch.long)  # è½¬ä¸º tensor
            else:
                merged_token_tensor = None
                lengths = None

            # æ£€æŸ¥ merged_token_tensor æ˜¯å¦ä¸º None
            if merged_token_tensor is None or lengths is None:
                fabric.print(f"Warning: merged_token_tensor or lengths is None at iter {state['iter_num']}. "
                           f"train_data keys: {list(train_data.keys()) if isinstance(train_data, dict) else 'not a dict'}. "
                           f"Skipping this batch.")
                state["iter_num"] += 1
                continue

            input_token = merged_token_tensor[:, :-1].contiguous()
            target_token = merged_token_tensor[:, 1:].contiguous()
            batch_size, seq_len = target_token.shape
            
            # è®¡ç®—ä½ç½® maskï¼šæ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆ target é•¿åº¦ = åŸå§‹é•¿åº¦ - 1
            valid_lens = (lengths - 1).clamp(min=1, max=seq_len)  # (B,)
            pos = torch.arange(seq_len, device=fabric.device).unsqueeze(0)  # (1, T)
            pad_mask = (pos < valid_lens.unsqueeze(1)).to(torch.float32)  # (B, T), True è¡¨ç¤ºæœ‰æ•ˆä½ç½®

            # print(f"input_token: {input_token.shape}, target_token: {target_token.shape}, pc: {pc.shape}, batch_size: {batch_size}, seq_len: {seq_len}, gradient_accumulation_steps: {gradient_accumulation_steps}")
            # print(f"pc[0]: {pc[0][:5]}, pc[1]: {pc[1][:5]}, pc[2]: {pc[2][:5]}, pc[3]: {pc[3][:5]}")
            is_accumulating = (state["iter_num"] + 1) % gradient_accumulation_steps != 0
            
            # ç›‘æ§ condition embeddings ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ¯ 500 æ­¥ï¼‰
            # monitor_condition = (state["iter_num"] % 500 == 0) and fabric.global_rank == 0
            
            with fabric.no_backward_sync(model, enabled=is_accumulating):
                logits = model(input_token, pc=pc, window_size=9000).logits
                
                # ä½¿ç”¨ä½ç½® mask è®¡ç®— lossï¼ˆä¸ä¾èµ– token idï¼‰
                # logits: (B, T, vocab_size), target_token: (B, T)
                per_token_loss = torch.nn.functional.cross_entropy(
                    logits.reshape(-1, logits.size(-1)),  # (B*T, vocab_size)
                    target_token.reshape(-1),  # (B*T,)
                    reduction='none'
                ).view(batch_size, seq_len)  # (B, T)
                
                # åªå¯¹æœ‰æ•ˆä½ç½®è®¡ç®— lossï¼ˆä½¿ç”¨ä½ç½® maskï¼‰
                masked_loss = per_token_loss * pad_mask  # (B, T)
                per_sample_loss = masked_loss.sum(dim=1) / valid_lens.to(masked_loss.dtype)  # (B,)
                loss = per_sample_loss.mean()  # scalar
                
                # ç›‘æ§ condition ä¿¡æ¯
                # if monitor_condition:
                #     with torch.no_grad():
                #         # è·å– condition embeddingsï¼ˆé€šè¿‡å®Œæ•´çš„ encode + decode æµç¨‹ï¼‰
                #         try:
                #             # è®¿é—® conditionerï¼ˆéœ€è¦å¤„ç† FSDP åŒ…è£…ï¼‰
                #             raw_model = model.module if hasattr(model, 'module') else model
                #             if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
                #                 # Stage 1: ç¼–ç ä¸º latent codes
                #                 latent_codes = raw_model.conditioner.encode(pc, sample_posterior=False)
                #                 # latent_codes: (bs, num_latents, embed_dim)
                                
                #                 # Stage 2: è§£ç ä¸ºç‰¹å¾
                #                 cond_embeds = raw_model.conditioner.decode(latent_codes)
                #                 # cond_embeds: (bs, num_latents, width)
                                
                #                 # åº”ç”¨é™é‡‡æ ·ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                #                 if hasattr(raw_model, 'condition_downsample_factor') and raw_model.condition_downsample_factor > 1:
                #                     from einops import rearrange
                #                     factor = raw_model.condition_downsample_factor
                #                     if hasattr(raw_model, 'condition_downsample') and raw_model.condition_downsample is not None:
                #                         # å¯å­¦ä¹ é™é‡‡æ · - ç¡®ä¿ dtype åŒ¹é…
                #                         target_dtype = next(raw_model.condition_downsample.parameters()).dtype
                #                         cond_embeds_downsampled = rearrange(cond_embeds, 'b (n f) d -> b n (f d)', f=factor)
                #                         cond_embeds_downsampled = cond_embeds_downsampled.to(target_dtype)
                #                         cond_embeds_downsampled = raw_model.condition_downsample(cond_embeds_downsampled)
                #                     else:
                #                         # å¹³å‡æ± åŒ–
                #                         cond_embeds_downsampled = rearrange(cond_embeds, 'b (n f) d -> b n f d', f=factor)
                #                         cond_embeds_downsampled = cond_embeds_downsampled.mean(dim=2)
                #                 else:
                #                     cond_embeds_downsampled = cond_embeds
                                
                #                 # Project åˆ° model dimensionï¼ˆç¡®ä¿ dtype å®Œå…¨åŒ¹é…ï¼‰
                #                 linear_dtype = raw_model.linear.weight.dtype
                #                 linear_device = raw_model.linear.weight.device
                #                 cond_embeds_downsampled = cond_embeds_downsampled.to(dtype=linear_dtype, device=linear_device)
                #                 cond_embeds_proj = raw_model.linear(cond_embeds_downsampled)
                                
                #                 condition_stats = {
                #                     "condition/latent_codes_mean": latent_codes.float().mean().item(),
                #                     "condition/latent_codes_std": latent_codes.float().std().item(),
                #                     "condition/decoded_mean": cond_embeds.float().mean().item(),
                #                     "condition/decoded_std": cond_embeds.float().std().item(),
                #                     "condition/proj_mean": cond_embeds_proj.float().mean().item(),
                #                     "condition/proj_std": cond_embeds_proj.float().std().item(),
                #                     "condition/num_tokens": cond_embeds_proj.shape[1],
                #                 }
                                
                #                 fabric.print(f"\n[Condition Stats @ iter {state['iter_num']}]")
                #                 fabric.print(f"  Latent codes: mean={condition_stats['condition/latent_codes_mean']:.4f}, "
                #                            f"std={condition_stats['condition/latent_codes_std']:.4f}")
                #                 fabric.print(f"  Decoded features: mean={condition_stats['condition/decoded_mean']:.4f}, "
                #                            f"std={condition_stats['condition/decoded_std']:.4f}")
                #                 fabric.print(f"  Projected: mean={condition_stats['condition/proj_mean']:.4f}, "
                #                            f"std={condition_stats['condition/proj_std']:.4f}")
                #                 fabric.print(f"  Num context tokens: {condition_stats['condition/num_tokens']}")
                                
                #                 fabric.log_dict(condition_stats, state["step_count"])
                #         except Exception as e:
                #             fabric.print(f"Warning: Failed to monitor condition stats: {e}")
                
                fabric.backward(loss / gradient_accumulation_steps)

            if not is_accumulating:
                # ========== åŒæ­¥ conditioner æ¢¯åº¦ï¼ˆå¦‚æœè§£é”è®­ç»ƒï¼‰ ==========
                # ç”±äº conditioner åœ¨ FSDP çš„ ignored_modules ä¸­ï¼Œå¤š GPU æ—¶æ¢¯åº¦ä¸ä¼šè‡ªåŠ¨åŒæ­¥
                # éœ€è¦æ‰‹åŠ¨è¿›è¡Œ all-reduce
                if not freeze_conditioner and fabric.world_size > 1:
                    raw_model = model.module if hasattr(model, 'module') else model
                    if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
                        for param in raw_model.conditioner.parameters():
                            if param.grad is not None:
                                # all-reduce æ¢¯åº¦å¹¶å–å¹³å‡
                                torch.distributed.all_reduce(param.grad, op=torch.distributed.ReduceOp.SUM)
                                param.grad.div_(fabric.world_size)
                
                fabric.clip_gradients(model, optimizer, max_norm=grad_clip)
                
                # ç›‘æ§ condition ç›¸å…³å±‚çš„æ¢¯åº¦ï¼ˆæ¯ 100 æ­¥ï¼‰
                if state["step_count"] % 100 == 0 and fabric.global_rank == 0:
                    condition_grad_stats = {}
                    cross_attn_grads = []
                    linear_grads = []
                    
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            grad_norm = param.grad.norm().item()
                            # ç›‘æ§ cross-attention å±‚
                            if 'cross_attn' in name:
                                # åªè®°å½•æƒé‡å±‚çš„æ¢¯åº¦ï¼Œè·³è¿‡å…·ä½“å‚æ•°åä»¥ä¿æŒç®€æ´
                                if 'weight' in name:
                                    cross_attn_grads.append(grad_norm)
                                    layer_type = name.split('.')[-2]  # q_proj, kv_proj, out_proj
                                    condition_grad_stats[f"grad/cross_attn_{layer_type}"] = grad_norm
                            # ç›‘æ§ condition projection å±‚
                            elif name.endswith('linear.weight') or name.endswith('linear.bias'):
                                linear_grads.append(grad_norm)
                                condition_grad_stats[f"grad/{name.split('.')[-1]}"] = grad_norm
                    
                    if condition_grad_stats:
                        # è®¡ç®—ç»Ÿè®¡é‡
                        if cross_attn_grads:
                            condition_grad_stats["grad/cross_attn_mean"] = sum(cross_attn_grads) / len(cross_attn_grads)
                            condition_grad_stats["grad/cross_attn_max"] = max(cross_attn_grads)
                        if linear_grads:
                            condition_grad_stats["grad/linear_mean"] = sum(linear_grads) / len(linear_grads)
                        
                        fabric.print(f"\n[Condition Gradient Monitor @ step {state['step_count']}]")
                        if cross_attn_grads:
                            fabric.print(f"  CrossAttention: mean={condition_grad_stats['grad/cross_attn_mean']:.6f}, "
                                       f"max={condition_grad_stats['grad/cross_attn_max']:.6f}, "
                                       f"count={len(cross_attn_grads)}")
                        if linear_grads:
                            fabric.print(f"  Linear projection: mean={condition_grad_stats['grad/linear_mean']:.6f}")
                        
                        # è®°å½•åˆ° wandb
                        fabric.log_dict(condition_grad_stats, state["step_count"])
                
                optimizer.step()
                optimizer.zero_grad()
                state["step_count"] += 1
            elif fabric.device.type == "xla":
                import torch_xla.core.xla_model as xm
                xm.mark_step()
            state["iter_num"] += 1


            total_lengths += input_token.size(1)
            t1 = time.perf_counter()
            elapsed_iters = max(1, state['iter_num'] - initial_iter)
            remaining_hours = (t1 - total_t0) / elapsed_iters * max(0, (max_iters - state['iter_num'])) / 3600
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆåŒ…å« CD lossï¼‰
            
            fabric.print(
                f"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, "
                f"iter: {idddx}/{len(train_dataloader)} , epoch: {state['epoch']}, gap: {maxL-minL}, lr: {lr}, iter time:"
                f" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}"
                f" remaining time: {remaining_hours:.2f} hours. "
                f" or {remaining_hours / 24:.2f} days. "
            )

            monitor.on_train_batch_end(
                state["iter_num"] * micro_batch_size,
                t1 - total_t0,
                fabric.world_size,
                state["step_count"],
                flops_per_batch=estimated_flops,
                lengths=total_lengths,
                train_loss = loss.item(),
                lr = lr,
                FWLoss = 0.0,
                cd_loss = 0.0,
            )

            # if val_dataloader is not None and not is_accumulating and state["step_count"] % eval_step_interval == 0:
            #     t0 = time.perf_counter()
            #     val_loss = validate(fabric, model, val_dataloader)
            #     t1 = time.perf_counter() - t0
            #     monitor.eval_end(t1)
            #     for i in range(num_extrapol):
            #         fabric.print(f"step {state['iter_num']}: val loss {val_loss[i]:.4f}, val time: {t1 * 1000:.2f}ms")
            #         fabric.log_dict({"metric/val_loss@"+str(i+1)+"x": val_loss[i].item(), "total_tokens": model.config.block_size * (state["iter_num"] + 1) * micro_batch_size * fabric.world_size}, state["step_count"])
            #         fabric.log_dict({"metric/val_ppl@"+str(i+1)+"x": math.exp(val_loss[i].item()), "total_tokens": model.config.block_size * (state["iter_num"] + 1) * micro_batch_size * fabric.world_size}, state["step_count"])
            #     fabric.barrier()

            if not is_accumulating and state["step_count"] % save_step_interval == 0:
                checkpoint_path = out_dir / f"iter-{state['iter_num']:06d}-ckpt.pth"
                fabric.print(f"Saving checkpoint to {str(checkpoint_path)!r}")
                save_checkpoint_with_conditioner(fabric, checkpoint_path, state)
                fabric.barrier()  # ç¡®ä¿æ‰€æœ‰ rank åŒæ­¥

            if state["iter_num"] >= max_iters:
                break

        state["epoch"] = epoch + 1
        if state["iter_num"] >= max_iters:
            break

    # ========== è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆ checkpoint ==========
    final_checkpoint_path = out_dir / f"iter-{state['iter_num']:06d}-final-ckpt.pth"
    fabric.print(f"\nğŸ Training finished! Saving final checkpoint to {str(final_checkpoint_path)!r}")
    save_checkpoint_with_conditioner(fabric, final_checkpoint_path, state)
    fabric.barrier()

# @torch.no_grad()
# def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:
#     fabric.print("Validating ...")
#     model.eval()
    
#     # è·å–åŸå§‹æ¨¡å‹å¹¶ä¿å­˜ conditioner çš„è®­ç»ƒçŠ¶æ€
#     raw_model = model.module if hasattr(model, 'module') else model
#     conditioner_was_training = False
#     if hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
#         conditioner_was_training = raw_model.conditioner.training
#         raw_model.conditioner.eval()

#     losses = torch.zeros(eval_iters, num_extrapol, device=fabric.device)
#     for k, val_data in enumerate(val_dataloader):
#         if k >= eval_iters:
#             break

#         # å¦‚æœæ˜¯ Sample_Dataset (dict with pc)ï¼Œéœ€è¦æå– pc
#         # å¦‚æœæ˜¯ PackedDataset (tensor only)ï¼Œpc=Noneï¼ˆæ— æ¡ä»¶éªŒè¯ï¼‰
#         pc = None
#         if isinstance(val_data, dict):
#             pc_list = val_data.get('pc', None) or val_data.get('pc_normal', None)
#             if pc_list is not None and len(pc_list) > 0:
#                 pc = torch.stack(pc_list, dim=0).to(fabric.device)
#             val_data = val_data.get('token_list_0', val_data)  # æå– token æ•°æ®

#         for i, length in enumerate([4096, 8192, 12288, 16384]):   #[2048, 4096, 8192, 16384]
#             input_ids = val_data[:, 0 : length].contiguous()
#             targets = val_data[:, 1 : length + 1].contiguous()
#             # ä¼ å…¥ pc å‚æ•°ï¼ˆå¯ä»¥æ˜¯ Noneï¼Œæ¨¡å‹ä¼šæ­£ç¡®å¤„ç†ï¼‰
#             logits = model(input_ids, pc=pc).logits
#             loss = chunked_cross_entropy(logits, targets, chunk_size=0)
#             losses[k,i] = loss.item()

#     out = losses.mean(0)
#     model.train()
    
#     # æ¢å¤ conditioner çš„è®­ç»ƒçŠ¶æ€ï¼ˆå¦‚æœä¹‹å‰æ˜¯è®­ç»ƒæ¨¡å¼ï¼‰
#     if conditioner_was_training and hasattr(raw_model, 'conditioner') and raw_model.conditioner is not None:
#         raw_model.conditioner.train()
    
#     return out


def create_dataloaders(
    batch_size: int,
    fabric,
    config_dict: Optional[dict] = None,
    seed: int = 12345,
) -> Tuple[DataLoader, DataLoader]:
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        fabric: Lightning Fabric å®ä¾‹
        config_dict: é…ç½®å­—å…¸ï¼ˆOmegaConf æ ¼å¼ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        seed: éšæœºç§å­
    
    Returns:
        train_dataloader, val_dataloader
    """
    def collate_as_list(batch):
        out = {}
        for item in batch:
            for k, v in item.items():
                out.setdefault(k, []).append(v)
        return out

    # ========== åŠ è½½æ•°æ®é›† ==========
    if config_dict is not None:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶åŠ è½½æ•°æ®é›†
        print("ğŸ“‚ Loading datasets from config...")
        train_dataset = load_dataset_from_config(config_dict, section="data_train")
        # try:
        #     val_dataset = load_dataset_from_config(config_dict, section="data_val")
        #     print('âœ… Validation dataset loaded')
        # except (ValueError, KeyError) as e:
        #     print(f'âš ï¸  No validation dataset found in config: {e}')
        #     val_dataset = None
    else:
        # å‘åå…¼å®¹ï¼šä½¿ç”¨æ—§çš„ç¡¬ç¼–ç æ–¹å¼
        print("âš ï¸  No config provided, using legacy dataset loading...")
        # è¿™é‡Œå¯ä»¥ä¿ç•™æ—§çš„é€»è¾‘ï¼Œæˆ–è€…æŠ›å‡ºé”™è¯¯
        raise ValueError(
            "config_dict is required. Please provide a config file with 'data_train' section. "
            "Example: python pretrain.py --config_path path/to/config.yaml"
        )

    sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=fabric.world_size,
        rank=fabric.global_rank,
        shuffle=True,
        drop_last=False,
    )
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        num_workers=8,  # å¢åŠ  worker æ•°é‡ï¼ˆåŸæ¥æ˜¯4ï¼‰
        pin_memory=True,
        collate_fn=collate_as_list,
        sampler=sampler,
        prefetch_factor=8,  # æ¯ä¸ª worker é¢„å–4ä¸ªæ‰¹æ¬¡ï¼ˆé»˜è®¤æ˜¯2ï¼‰
        persistent_workers=False,  
    )

    # è¿”å›æ•°æ®åŠ è½½å™¨ï¼ˆéªŒè¯é›†å¯èƒ½ä¸º Noneï¼‰
    return train_dataloader, None

# learning rate decay scheduler (cosine with linear warmup)
def get_lr(it: int, warmup_iters: int, max_iters: int) -> float:
    if it < warmup_iters:
        return learning_rate * it / warmup_iters
    if it > max_iters:
        return min_lr
    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (learning_rate - min_lr)

if __name__ == "__main__":
    # torch.backends.cuda.enable_flash_sdp(False)
    torch.set_float32_matmul_precision("high")
    from jsonargparse import CLI
    CLI(setup)
