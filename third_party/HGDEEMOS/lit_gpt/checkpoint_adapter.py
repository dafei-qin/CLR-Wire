"""
Checkpointé€‚é…å·¥å…·ï¼šå¤„ç†æ¨¡å‹ç»“æ„å˜åŒ–åçš„æƒé‡åŠ è½½
ç”¨äºä»æ—§çš„reshapeæ–¹æ¡ˆ(256, 1024)è¿ç§»åˆ°æ–°çš„ç›´æ¥projectæ–¹æ¡ˆ(4096, 64)
"""
import torch
import torch.nn as nn
from typing import Dict, List, Optional
from pathlib import Path


def load_checkpoint_skip_incompatible(
    model: nn.Module, 
    checkpoint_path: str,
    skip_keys: Optional[List[str]] = None,
    verbose: bool = True
) -> nn.Module:
    """
    åŠ è½½checkpointï¼Œè‡ªåŠ¨è·³è¿‡ä¸å…¼å®¹çš„å±‚
    
    Args:
        model: ç›®æ ‡æ¨¡å‹
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
        skip_keys: éœ€è¦è·³è¿‡çš„é”®åˆ—è¡¨ï¼ˆé»˜è®¤è·³è¿‡linearå’Œnormï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        åŠ è½½æƒé‡åçš„æ¨¡å‹
    """
    if skip_keys is None:
        # é»˜è®¤è·³è¿‡ condition projection ç›¸å…³çš„å±‚
        skip_keys = ['linear.weight', 'linear.bias', 'norm.weight', 'norm.bias']
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # å¦‚æœcheckpointæ˜¯å­—å…¸ä¸”åŒ…å«'model'é”®
    if isinstance(checkpoint, dict) and 'model' in checkpoint:
        state_dict = checkpoint['model']
    else:
        state_dict = checkpoint
    
    # è·å–æ¨¡å‹å½“å‰çš„ state_dict
    model_state = model.state_dict()
    
    # è¿‡æ»¤æ‰éœ€è¦è·³è¿‡çš„é”®å’Œå½¢çŠ¶ä¸åŒ¹é…çš„é”®
    compatible_state = {}
    incompatible_keys = []
    shape_mismatch_keys = []
    
    for k, v in state_dict.items():
        # æ£€æŸ¥æ˜¯å¦åœ¨è·³è¿‡åˆ—è¡¨ä¸­
        if any(skip_key in k for skip_key in skip_keys):
            incompatible_keys.append(k)
            continue
        
        # æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨äºæ¨¡å‹ä¸­
        if k not in model_state:
            incompatible_keys.append(k)
            continue
        
        # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
        if v.shape != model_state[k].shape:
            shape_mismatch_keys.append(f"{k}: {v.shape} -> {model_state[k].shape}")
            continue
        
        compatible_state[k] = v
    
    # åŠ è½½å…¼å®¹çš„æƒé‡
    missing_keys, unexpected_keys = model.load_state_dict(compatible_state, strict=False)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ CheckpointåŠ è½½æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"âœ… æˆåŠŸåŠ è½½: {len(compatible_state)} ä¸ªå‚æ•°")
        print(f"âš ï¸  è·³è¿‡(åœ¨skip_keysä¸­): {len(incompatible_keys)} ä¸ª")
        print(f"âš ï¸  å½¢çŠ¶ä¸åŒ¹é…: {len(shape_mismatch_keys)} ä¸ª")
        print(f"â“ ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
        print(f"â“ å¤šä½™çš„é”®: {len(unexpected_keys)} ä¸ª")
        
        if incompatible_keys and len(incompatible_keys) <= 10:
            print(f"\nè·³è¿‡çš„é”®: {incompatible_keys}")
        
        if shape_mismatch_keys:
            print(f"\nå½¢çŠ¶ä¸åŒ¹é…çš„é”®:")
            for key_info in shape_mismatch_keys[:5]:
                print(f"  - {key_info}")
            if len(shape_mismatch_keys) > 5:
                print(f"  ... è¿˜æœ‰ {len(shape_mismatch_keys) - 5} ä¸ª")
        
        print(f"\nğŸ”„ æœªåŠ è½½çš„å±‚å°†ä¿æŒéšæœºåˆå§‹åŒ–çŠ¶æ€")
        print(f"ğŸ’¡ å»ºè®®ï¼šå…ˆç”¨å¤§å­¦ä¹ ç‡è®­ç»ƒè¿™äº›å±‚ï¼Œå†è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒ")
        print(f"{'='*60}\n")
    
    return model


def create_staged_optimizer(
    model: nn.Module,
    stage: str = "warmup",
    warmup_lr: float = 1e-3,
    finetune_lr: float = 1e-5,
    weight_decay: float = 0.01
) -> torch.optim.Optimizer:
    """
    åˆ›å»ºåˆ†é˜¶æ®µçš„ä¼˜åŒ–å™¨
    
    Args:
        model: æ¨¡å‹
        stage: "warmup" æˆ– "finetune"
        warmup_lr: warmupé˜¶æ®µçš„å­¦ä¹ ç‡ï¼ˆåªè®­ç»ƒæ–°åˆå§‹åŒ–çš„å±‚ï¼‰
        finetune_lr: finetuneé˜¶æ®µçš„å­¦ä¹ ç‡ï¼ˆè®­ç»ƒæ‰€æœ‰å±‚ï¼‰
        weight_decay: æƒé‡è¡°å‡
    
    Returns:
        ä¼˜åŒ–å™¨
    """
    if stage == "warmup":
        # ç¬¬ä¸€é˜¶æ®µï¼šåªè®­ç»ƒ linear å’Œ norm å±‚
        print("\nğŸ”¥ Warmupé˜¶æ®µï¼šåªè®­ç»ƒ condition projection å±‚")
        trainable_params = []
        frozen_params = 0
        
        for name, param in model.named_parameters():
            if 'linear' in name or 'norm' in name:
                param.requires_grad = True
                trainable_params.append(param)
                print(f"  âœ“ {name}: å¯è®­ç»ƒ")
            else:
                param.requires_grad = False
                frozen_params += 1
        
        print(f"\nğŸ“Š å¯è®­ç»ƒå‚æ•°: {len(trainable_params)}")
        print(f"ğŸ“Š å†»ç»“å‚æ•°: {frozen_params}")
        print(f"ğŸ“Š å­¦ä¹ ç‡: {warmup_lr}\n")
        
        return torch.optim.AdamW(trainable_params, lr=warmup_lr, weight_decay=weight_decay)
    
    elif stage == "finetune":
        # ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ‰€æœ‰å±‚
        print("\nğŸ”¥ Finetuneé˜¶æ®µï¼šè®­ç»ƒæ‰€æœ‰å±‚")
        
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = True
        
        # å¯ä»¥å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
        param_groups = [
            {
                'params': [p for n, p in model.named_parameters() 
                          if 'linear' in n or 'norm' in n],
                'lr': finetune_lr * 2,  # conditionå±‚ç”¨ç¨å¤§çš„å­¦ä¹ ç‡
                'name': 'condition_projection'
            },
            {
                'params': [p for n, p in model.named_parameters() 
                          if 'linear' not in n and 'norm' not in n],
                'lr': finetune_lr,
                'name': 'backbone'
            }
        ]
        
        print(f"ğŸ“Š Conditionå±‚å­¦ä¹ ç‡: {finetune_lr * 2}")
        print(f"ğŸ“Š ä¸»å¹²ç½‘ç»œå­¦ä¹ ç‡: {finetune_lr}\n")
        
        return torch.optim.AdamW(param_groups, weight_decay=weight_decay)
    
    else:
        raise ValueError(f"Unknown stage: {stage}. Use 'warmup' or 'finetune'")


def save_training_config(save_path: str, config: Dict):
    """ä¿å­˜è®­ç»ƒé…ç½®ï¼Œæ–¹ä¾¿è¿½è¸ªå®éªŒ"""
    import json
    with open(save_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"ğŸ’¾ è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {save_path}")


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    print("""
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    
    # 1. åŠ è½½æ—§checkpointï¼Œè·³è¿‡ä¸å…¼å®¹çš„å±‚
    from lit_gpt.model import GPT
    from lit_gpt.config import Config
    from lit_gpt.checkpoint_adapter import load_checkpoint_skip_incompatible, create_staged_optimizer
    
    config = Config.from_name("your_config")
    model = GPT(config)
    
    # åŠ è½½checkpoint
    model = load_checkpoint_skip_incompatible(
        model, 
        "path/to/old_checkpoint.pth",
        verbose=True
    )
    
    # 2. ç¬¬ä¸€é˜¶æ®µï¼šWarmupè®­ç»ƒï¼ˆ1-2ä¸ªepochï¼‰
    optimizer_warmup = create_staged_optimizer(model, stage="warmup", warmup_lr=1e-3)
    
    for epoch in range(2):
        # è®­ç»ƒå¾ªç¯...
        pass
    
    # 3. ç¬¬äºŒé˜¶æ®µï¼šç«¯åˆ°ç«¯å¾®è°ƒ
    optimizer_finetune = create_staged_optimizer(model, stage="finetune", finetune_lr=1e-5)
    
    for epoch in range(remaining_epochs):
        # è®­ç»ƒå¾ªç¯...
        pass
    """)

