resume_training: true
from_start: false

epochs: 5000
batch_size: 512
lr: 0.0001
num_workers: 4
num_gpus: 8

grad_accum_every: 1
ema_update_every: 10
max_grad_norm: 1.0

val_every_epoch: 10
save_every_epoch: 50
log_every_step: 50

use_wandb_tracking: true
wandb_project_name: DIT_SIMPLE
wandb_run_name: 1128_dit_simple_vpred_fix_mask_loss_large_params_weight

data:
    train_latent_dir: ../data/logan_jsons_latent_1126/abc
    train_pc_dir: ../data/logan_jsons_michel_latent/abc
    val_latent_dir: ../data/logan_jsons_latent_1126/abc_test
    val_pc_dir: ../data/logan_jsons_michel_latent/abc_test
    surface_latent_dim: 128
    max_num_surfaces: 32
    train_num: -1
    val_num: -1
    replica: 1
    replica_val: 1
    log_scale: true

trainer:
    use_logvar: false
    prediction_type: v_prediction
    num_inference_timesteps: 50


model:
    name: dit_simple_sigmoid_scale
    trainer_name: dit_v1
    input_dim: 139
    cond_dim: 768
    output_dim: 139
    latent_dim: 384
    num_layers: 12
    num_heads: 6
    checkpoint_folder: checkpoints/1128_dit_simple_vpred_fix_mask_loss_large_params_weight
    checkpoint_file_name: model-16.pt

vae:
    param_raw_dim: [17, 18, 19, 18, 19]
    checkpoint_file_name: checkpoints/train_1126_vae_1_canonical_logvar/model-02.pt

loss:
    weight_valid: 0.01
    weight_params: 10
