resume_training: false
from_start: false

epochs: 8000
batch_size: 512
lr: 0.0001
num_workers: 4
num_gpus: 8

grad_accum_every: 1
ema_update_every: 10
max_grad_norm: 1.0

val_every_epoch: 10
save_every_epoch: 50
log_every_step: 50

use_wandb_tracking: true
wandb_project_name: DIT_SIMPLE
wandb_run_name: 1204_dit_v2_simple_vae_pred_closed_large_rts

data:
    name: src.dataset.dataset_latent.LatentDataset
    params:
        latent_dir: ../data/logan_jsons_latent_pred_closed/abc
        pc_dir: ../data/logan_jsons_michel_latent/abc
        latent_dim: 128
        num_data: -1
        log_scale: true
        replica: 1
        max_num_surfaces: 32
        val_latent_dir: ../data/logan_jsons_latent_pred_closed/abc_test
        val_pc_dir: ../data/logan_jsons_michel_latent/abc_test

data_val:
    name: src.dataset.dataset_latent.LatentDataset
    params:
        latent_dir: ../data/logan_jsons_latent_pred_closed/abc_test
        pc_dir: ../data/logan_jsons_michel_latent/abc_test
        latent_dim: 128
        num_data: -1
        log_scale: true
        replica: 1
        max_num_surfaces: 32


trainer:
    use_logvar: false
    prediction_type: v_prediction
    num_inference_timesteps: 50


model:
    # name: dit_v2
    name: src.dit.simple_surface_decoder_v2.SimpleSurfaceDecoder
    trainer_name: dit_v1
    checkpoint_folder: checkpoints/1204_dit_v2_simple_vae_pred_closed_large_rts
    checkpoint_file_name: model-22.pt
    params:
        input_dim: 139
        cond_dim: 768
        output_dim: 139
        latent_dim: 384
        num_layers: 12
        num_heads: 6

vae:
    param_raw_dim: [17, 18, 19, 18, 19]
    checkpoint_file_name: checkpoints/train_1201_vae_1_canonical_var_l2norm_pred_closed/model-03.pt

loss:
    weight_valid: 0.01
    weight_params: 5.0
    weight_rotations: 5.0
    weight_scales: 5.0
    weight_shifts: 5.0
