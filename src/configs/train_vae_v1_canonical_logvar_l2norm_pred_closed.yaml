resume_training: false
from_start: false

epochs: 200
batch_size: 256
lr: 0.0005
num_workers: 0
num_workers_val: 8
num_gpus: 6

grad_accum_every: 1
ema_update_every: 10
max_grad_norm: 1.0

val_every_epoch: 2
save_every_epoch: 20
log_every_step: 50

use_wandb_tracking: true
wandb_project_name: VAE_V1
wandb_run_name: train_1201_vae_1_canonical_var_l2norm_pred_closed

trainer:
    use_logvar: true
    u_closed_pos_weight: 10.0
    v_closed_pos_weight: 50.0

data:
    train_json_dir: ../data/logan_jsons/abc/
    val_json_dir: ../data/logan_jsons/abc_test
    max_num_surfaces: 500
    canonical: true
    use_weighted_sampling: false
    use_cache: true
    cache_path: ../data/logan_jsons_cache/abc_cache_w_closed.npz
    weight_path: assets/logan_jsons_weights.npy


model:
    # depth: 16
    # dim: 512
    # heads: 8
    # surface_res: 32
    # num_types: 6
    # num_nearby: 20
    # surface_dim: 256
    # surface_enc_block_out_channels: [16, 32, 64, 128]
    name: vae_v2
    pred_is_closed: true
    param_raw_dim: [17, 18, 19, 18, 19]
    checkpoint_folder: checkpoints/train_1201_vae_1_canonical_var_l2norm_pred_closed
    checkpoint_file_name: model-05.pt

loss:
    recon_weight: 10.0
    cls_weight: 1.0
    kl_weight: 0.01
    l2norm_weight: 0.01
    is_closed_weight: 1.0
    kl_annealing_steps: 20000  # Number of steps to anneal KL weight from 0 to kl_weight
    kl_free_bits: 0.25  # Free bits per latent dimension (in nats)